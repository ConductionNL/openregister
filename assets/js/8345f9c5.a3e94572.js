"use strict";(self.webpackChunkopen_catalogi_docs=self.webpackChunkopen_catalogi_docs||[]).push([[1938],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>c});var r=i(96540);const s={},l=r.createContext(s);function t(e){const n=r.useContext(l);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(l.Provider,{value:n},e.children)}},84674:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"Integrations/huggingface","title":"Hugging Face Integration","description":"Integrate OpenRegister with Hugging Face Text Generation Inference (TGI) or vLLM to run Mistral and other Hugging Face models locally with an OpenAI-compatible API.","source":"@site/docs/Integrations/huggingface.md","sourceDirName":"Integrations","slug":"/Integrations/huggingface","permalink":"/docs/Integrations/huggingface","draft":false,"unlisted":false,"editUrl":"https://github.com/conductionnl/openregister/tree/main/website/docs/Integrations/huggingface.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Dolphin LLM Model Integration","permalink":"/docs/Integrations/dolphin"},"next":{"title":"Mistral LLM Model Integration","permalink":"/docs/Integrations/mistral"}}');var s=i(74848),l=i(28453);const t={},c="Hugging Face Integration",o={},d=[{value:"Overview",id:"overview",level:2},{value:"Comparison",id:"comparison",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Option 1: Text Generation Inference (TGI) - Recommended",id:"option-1-text-generation-inference-tgi---recommended",level:3},{value:"Option 2: vLLM - Alternative",id:"option-2-vllm---alternative",level:3},{value:"Configuration Details",id:"configuration-details",level:2},{value:"TGI Service Configuration",id:"tgi-service-configuration",level:3},{value:"vLLM Service Configuration",id:"vllm-service-configuration",level:3},{value:"Available Models",id:"available-models",level:2},{value:"Recommended Models for OpenRegister",id:"recommended-models-for-openregister",level:3},{value:"Changing the Model",id:"changing-the-model",level:3},{value:"API Usage",id:"api-usage",level:2},{value:"Testing the API",id:"testing-the-api",level:3},{value:"Using with LLPhant (PHP)",id:"using-with-llphant-php",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"1. AI Chat",id:"1-ai-chat",level:3},{value:"2. RAG (Retrieval Augmented Generation)",id:"2-rag-retrieval-augmented-generation",level:3},{value:"3. Function Calling",id:"3-function-calling",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Container Won&#39;t Start",id:"container-wont-start",level:3},{value:"Model Download Fails",id:"model-download-fails",level:3},{value:"Connection Errors from OpenRegister",id:"connection-errors-from-openregister",level:3},{value:"Slow Performance",id:"slow-performance",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Concurrent Requests",id:"concurrent-requests",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Support",id:"support",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"hugging-face-integration",children:"Hugging Face Integration"})}),"\n",(0,s.jsx)(n.p,{children:"Integrate OpenRegister with Hugging Face Text Generation Inference (TGI) or vLLM to run Mistral and other Hugging Face models locally with an OpenAI-compatible API."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Hugging Face provides two options for running local LLMs with OpenAI-compatible APIs:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Text Generation Inference (TGI)"}),": Official Hugging Face solution, optimized for production"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"vLLM"}),": Alternative with better throughput, full OpenAI API compatibility"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Both provide:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"OpenAI-Compatible API"})," - Drop-in replacement for OpenAI"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Privacy-First"})," - All data stays local"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Cost-Free"})," - No API fees"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Fast"})," - Optimized inference engines"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Flexible"})," - Choose any Hugging Face model"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"comparison",children:"Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"TGI"}),(0,s.jsx)(n.th,{children:"vLLM"}),(0,s.jsx)(n.th,{children:"Ollama"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"OpenAI API"})}),(0,s.jsx)(n.td,{children:"\u2705 Yes (v1.4.0+)"}),(0,s.jsx)(n.td,{children:"\u2705 Yes"}),(0,s.jsx)(n.td,{children:"\u274c No"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Speed"})}),(0,s.jsx)(n.td,{children:"\u26a1\u26a1 Fast"}),(0,s.jsx)(n.td,{children:"\u26a1\u26a1\u26a1 Very Fast"}),(0,s.jsx)(n.td,{children:"\u26a1 Good"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Models"})}),(0,s.jsx)(n.td,{children:"Hugging Face"}),(0,s.jsx)(n.td,{children:"Hugging Face"}),(0,s.jsx)(n.td,{children:"Curated list"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Setup"})}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Easy"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Memory"})}),(0,s.jsx)(n.td,{children:"8-16GB"}),(0,s.jsx)(n.td,{children:"8-16GB"}),(0,s.jsx)(n.td,{children:"8-16GB"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Use Case"})}),(0,s.jsx)(n.td,{children:"Production"}),(0,s.jsx)(n.td,{children:"High throughput"}),(0,s.jsx)(n.td,{children:"Simple setup"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Nextcloud 28+ with OpenRegister installed"}),"\n",(0,s.jsx)(n.li,{children:"Docker and Docker Compose"}),"\n",(0,s.jsx)(n.li,{children:"GPU recommended (8GB+ VRAM) for optimal performance"}),"\n",(0,s.jsx)(n.li,{children:"At least 16GB RAM for larger models"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsx)(n.h3,{id:"option-1-text-generation-inference-tgi---recommended",children:"Option 1: Text Generation Inference (TGI) - Recommended"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pros"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Official Hugging Face solution"}),"\n",(0,s.jsx)(n.li,{children:"Well-maintained and documented"}),"\n",(0,s.jsx)(n.li,{children:"Optimized for production"}),"\n",(0,s.jsx)(n.li,{children:"Automatic quantization"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Installation"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Start TGI with Mistral\ndocker-compose -f docker-compose.huggingface.yml up -d tgi-mistral\n\n# Wait for model download (~15GB for Mistral 7B)\ndocker logs -f openregister-tgi-mistral\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Configuration in OpenRegister"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Navigate to ",(0,s.jsx)(n.strong,{children:"Settings"})," \u2192 ",(0,s.jsx)(n.strong,{children:"OpenRegister"})," \u2192 ",(0,s.jsx)(n.strong,{children:"LLM Configuration"})]}),"\n",(0,s.jsxs)(n.li,{children:["Select ",(0,s.jsx)(n.strong,{children:"OpenAI"})," as provider (TGI is OpenAI-compatible)"]}),"\n",(0,s.jsxs)(n.li,{children:["Configure:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Base URL"}),": ",(0,s.jsx)(n.code,{children:"http://tgi-mistral:80"})," (from Nextcloud container)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model"}),": ",(0,s.jsx)(n.code,{children:"mistral-7b-instruct"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"API Key"}),": ",(0,s.jsx)(n.code,{children:"dummy"})," (not used for local)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"option-2-vllm---alternative",children:"Option 2: vLLM - Alternative"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pros"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Faster inference"}),"\n",(0,s.jsx)(n.li,{children:"Better throughput for multiple requests"}),"\n",(0,s.jsx)(n.li,{children:"Full OpenAI API compatibility"}),"\n",(0,s.jsx)(n.li,{children:"PagedAttention optimization"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Installation"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Start vLLM with Mistral\ndocker-compose -f docker-compose.huggingface.yml up -d vllm-mistral\n\n# Wait for model download\ndocker logs -f openregister-vllm-mistral\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Configuration in OpenRegister"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Navigate to ",(0,s.jsx)(n.strong,{children:"Settings"})," \u2192 ",(0,s.jsx)(n.strong,{children:"OpenRegister"})," \u2192 ",(0,s.jsx)(n.strong,{children:"LLM Configuration"})]}),"\n",(0,s.jsxs)(n.li,{children:["Select ",(0,s.jsx)(n.strong,{children:"OpenAI"})," as provider"]}),"\n",(0,s.jsxs)(n.li,{children:["Configure:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Base URL"}),": ",(0,s.jsx)(n.code,{children:"http://vllm-mistral:8000"})," (from Nextcloud container)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model"}),": ",(0,s.jsx)(n.code,{children:"mistral-7b-instruct"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"API Key"}),": ",(0,s.jsx)(n.code,{children:"dummy"})," (not used for local)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"configuration-details",children:"Configuration Details"}),"\n",(0,s.jsx)(n.h3,{id:"tgi-service-configuration",children:"TGI Service Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'tgi-mistral:\n  image: ghcr.io/huggingface/text-generation-inference:latest\n  container_name: openregister-tgi-mistral\n  restart: always\n  ports:\n    - "8081:80"\n  volumes:\n    - tgi-models:/data\n  environment:\n    - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\n    - MAX_INPUT_LENGTH=4096\n    - MAX_TOTAL_TOKENS=8192\n    - MAX_CONCURRENT_REQUESTS=128\n  deploy:\n    resources:\n      limits:\n        memory: 16G\n      reservations:\n        memory: 8G\n        devices:\n          - driver: nvidia\n            count: all\n            capabilities: [gpu]\n'})}),"\n",(0,s.jsx)(n.h3,{id:"vllm-service-configuration",children:"vLLM Service Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'vllm-mistral:\n  image: vllm/vllm-openai:latest\n  container_name: openregister-vllm-mistral\n  restart: always\n  ports:\n    - "8082:8000"\n  volumes:\n    - vllm-models:/root/.cache/huggingface\n  environment:\n    - MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.1\n    - TENSOR_PARALLEL_SIZE=1\n    - GPU_MEMORY_UTILIZATION=0.9\n    - SERVED_MODEL_NAME=mistral-7b-instruct\n'})}),"\n",(0,s.jsx)(n.h2,{id:"available-models",children:"Available Models"}),"\n",(0,s.jsx)(n.h3,{id:"recommended-models-for-openregister",children:"Recommended Models for OpenRegister"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Size"}),(0,s.jsx)(n.th,{children:"Use Case"}),(0,s.jsx)(n.th,{children:"Memory Required"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Mistral-7B-Instruct-v0.2"})}),(0,s.jsx)(n.td,{children:"7B"}),(0,s.jsx)(n.td,{children:"General purpose, RAG"}),(0,s.jsx)(n.td,{children:"16GB"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Mixtral-8x7B-Instruct"})}),(0,s.jsx)(n.td,{children:"47B"}),(0,s.jsx)(n.td,{children:"High quality, complex"}),(0,s.jsx)(n.td,{children:"48GB+"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Llama-3-8B-Instruct"})}),(0,s.jsx)(n.td,{children:"8B"}),(0,s.jsx)(n.td,{children:"General purpose"}),(0,s.jsx)(n.td,{children:"16GB"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Phi-3-mini-instruct"})}),(0,s.jsx)(n.td,{children:"3.8B"}),(0,s.jsx)(n.td,{children:"Fast, lightweight"}),(0,s.jsx)(n.td,{children:"8GB"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Qwen2-7B-Instruct"})}),(0,s.jsx)(n.td,{children:"7B"}),(0,s.jsx)(n.td,{children:"Multilingual, code"}),(0,s.jsx)(n.td,{children:"16GB"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"changing-the-model",children:"Changing the Model"}),"\n",(0,s.jsxs)(n.p,{children:["Edit ",(0,s.jsx)(n.code,{children:"docker-compose.huggingface.yml"}),":"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"For TGI"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"tgi-mistral:\n  environment:\n    - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2  # Change this\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"For vLLM"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"vllm-mistral:\n  environment:\n    - MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2  # Change this\n  command:\n    - --model\n    - mistralai/Mistral-7B-Instruct-v0.2  # Change this too\n"})}),"\n",(0,s.jsx)(n.p,{children:"Then restart the service:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker-compose -f docker-compose.huggingface.yml restart tgi-mistral\n# or\ndocker-compose -f docker-compose.huggingface.yml restart vllm-mistral\n"})}),"\n",(0,s.jsx)(n.h2,{id:"api-usage",children:"API Usage"}),"\n",(0,s.jsx)(n.h3,{id:"testing-the-api",children:"Testing the API"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"TGI (port 8081)"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl http://localhost:8081/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "mistral-7b-instruct",\n    "messages": [\n      {"role": "user", "content": "Hello! What is the capital of France?"}\n    ],\n    "max_tokens": 100\n  }\'\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"vLLM (port 8082)"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl http://localhost:8082/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "mistral-7b-instruct",\n    "messages": [\n      {"role": "user", "content": "Hello! What is the capital of France?"}\n    ],\n    "max_tokens": 100\n  }\'\n'})}),"\n",(0,s.jsx)(n.h3,{id:"using-with-llphant-php",children:"Using with LLPhant (PHP)"}),"\n",(0,s.jsx)(n.p,{children:"LLPhant's OpenAI client can point to TGI/vLLM:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-php",children:"use LLPhant\\Chat\\OpenAIChat;\nuse OpenAI\\Client;\n\n// Create OpenAI client pointing to TGI/vLLM\n$client = Client::factory()\n    ->withBaseUri('http://tgi-mistral:80')  // or http://vllm-mistral:8000\n    ->withHttpHeader('Content-Type', 'application/json')\n    ->make();\n\n// Use with LLPhant\n$chat = new OpenAIChat($client);\n$response = $chat->generateText('What is the capital of France?');\n"})}),"\n",(0,s.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,s.jsx)(n.h3,{id:"1-ai-chat",children:"1. AI Chat"}),"\n",(0,s.jsx)(n.p,{children:"Enable conversational AI using local models:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Configure TGI or vLLM"}),"\n",(0,s.jsx)(n.li,{children:"Set OpenAI provider with local base URL"}),"\n",(0,s.jsx)(n.li,{children:"Use chat features in OpenRegister"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-rag-retrieval-augmented-generation",children:"2. RAG (Retrieval Augmented Generation)"}),"\n",(0,s.jsx)(n.p,{children:"Answer questions using your data:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Configure embedding model (separate from chat)"}),"\n",(0,s.jsx)(n.li,{children:"Vectorize your objects and files"}),"\n",(0,s.jsx)(n.li,{children:"Ask questions - AI retrieves relevant context"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-function-calling",children:"3. Function Calling"}),"\n",(0,s.jsx)(n.p,{children:"Use Mistral with OpenRegister's function calling:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Search objects"}),"\n",(0,s.jsx)(n.li,{children:"Create objects"}),"\n",(0,s.jsx)(n.li,{children:"Update objects"}),"\n",(0,s.jsx)(n.li,{children:"Query registers"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"container-wont-start",children:"Container Won't Start"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check logs\ndocker logs openregister-tgi-mistral\n# or\ndocker logs openregister-vllm-mistral\n\n# Common issues:\n# 1. Port already in use\nsudo lsof -i :8081  # TGI\nsudo lsof -i :8082  # vLLM\n\n# 2. Insufficient memory\ndocker stats openregister-tgi-mistral\n\n# 3. GPU not available\ndocker exec openregister-tgi-mistral nvidia-smi\n"})}),"\n",(0,s.jsx)(n.h3,{id:"model-download-fails",children:"Model Download Fails"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check internet connection\ndocker exec openregister-tgi-mistral ping -c 3 huggingface.co\n\n# For gated models, set Hugging Face token:\n# Edit docker-compose.huggingface.yml:\nenvironment:\n  - HUGGING_FACE_HUB_TOKEN=your_token_here\n"})}),"\n",(0,s.jsx)(n.h3,{id:"connection-errors-from-openregister",children:"Connection Errors from OpenRegister"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": OpenRegister can't connect to TGI/vLLM."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Verify base URL uses container name: ",(0,s.jsx)(n.code,{children:"http://tgi-mistral:80"})]}),"\n",(0,s.jsx)(n.li,{children:"Check containers are on same Docker network"}),"\n",(0,s.jsxs)(n.li,{children:["Test connection from Nextcloud container:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker exec <nextcloud-container> curl http://tgi-mistral:80/health\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"slow-performance",children:"Slow Performance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Use GPU acceleration (10-100x faster)"}),"\n",(0,s.jsx)(n.li,{children:"Choose smaller model (3B instead of 7B)"}),"\n",(0,s.jsxs)(n.li,{children:["Increase ",(0,s.jsx)(n.code,{children:"MAX_CONCURRENT_REQUESTS"})," for TGI"]}),"\n",(0,s.jsxs)(n.li,{children:["Adjust ",(0,s.jsx)(n.code,{children:"GPU_MEMORY_UTILIZATION"})," for vLLM"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,s.jsx)(n.p,{children:"For best performance, use GPU:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"deploy:\n  resources:\n    devices:\n      - driver: nvidia\n        count: all\n        capabilities: [gpu]\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Performance Gain"}),": 10-100x faster inference with GPU"]}),"\n",(0,s.jsx)(n.h3,{id:"concurrent-requests",children:"Concurrent Requests"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"TGI"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"environment:\n  - MAX_CONCURRENT_REQUESTS=128  # Increase for more parallel requests\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"vLLM"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"environment:\n  - GPU_MEMORY_UTILIZATION=0.9  # Use 90% of GPU memory\n"})}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/text-generation-inference",children:"Hugging Face TGI Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.vllm.ai",children:"vLLM Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/docs/Integrations/mistral",children:"Mistral Model Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/docs/features/rag-implementation",children:"RAG Implementation"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"support",children:"Support"}),"\n",(0,s.jsx)(n.p,{children:"For issues specific to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TGI setup"}),": Check ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/text-generation-inference",children:"TGI Documentation"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"vLLM setup"}),": Check ",(0,s.jsx)(n.a,{href:"https://docs.vllm.ai",children:"vLLM Documentation"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OpenRegister integration"}),": OpenRegister GitHub issues"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}}}]);