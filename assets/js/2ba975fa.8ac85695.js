"use strict";(self.webpackChunkopen_catalogi_docs=self.webpackChunkopen_catalogi_docs||[]).push([[1058],{28453:(e,n,l)=>{l.d(n,{R:()=>o,x:()=>t});var s=l(96540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}},99401:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"development/ollama","title":"Ollama Setup and Configuration","description":"Guide to setting up and configuring Ollama for OpenRegister AI features","source":"@site/docs/development/ollama.md","sourceDirName":"development","slug":"/development/ollama","permalink":"/docs/development/ollama","draft":false,"unlisted":false,"editUrl":"https://github.com/conductionnl/openregister/tree/main/website/docs/development/ollama.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Ollama Setup and Configuration","sidebar_position":3,"description":"Guide to setting up and configuring Ollama for OpenRegister AI features","keywords":["Open Register","Ollama","LLM","GPU","AI"]},"sidebar":"tutorialSidebar","previous":{"title":"Performance Optimization","permalink":"/docs/development/performance-optimization"},"next":{"title":"External App Cache Optimization","permalink":"/docs/development/external-app-optimization"}}');var r=l(74848),i=l(28453);const o={title:"Ollama Setup and Configuration",sidebar_position:3,description:"Guide to setting up and configuring Ollama for OpenRegister AI features",keywords:["Open Register","Ollama","LLM","GPU","AI"]},t="Using Ollama with OpenRegister",a={},d=[{value:"What is Ollama?",id:"what-is-ollama",level:2},{value:"Technical Implementation",id:"technical-implementation",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"1. Start Ollama Container",id:"1-start-ollama-container",level:3},{value:"2. Pull a Model",id:"2-pull-a-model",level:3},{value:"3. Configure OpenRegister",id:"3-configure-openregister",level:3},{value:"4. Pull Embedding Model (for RAG)",id:"4-pull-embedding-model-for-rag",level:3},{value:"Configuration Details",id:"configuration-details",level:2},{value:"Ollama Service Configuration",id:"ollama-service-configuration",level:3},{value:"Accessing Ollama",id:"accessing-ollama",level:3},{value:"Model Naming",id:"model-naming",level:3},{value:"Recommended Models",id:"recommended-models",level:2},{value:"For Chat &amp; Agents",id:"for-chat--agents",level:3},{value:"For Embeddings (RAG)",id:"for-embeddings-rag",level:3},{value:"GPU Support",id:"gpu-support",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Verify WSL GPU Support (Windows + WSL)",id:"verify-wsl-gpu-support-windows--wsl",level:3},{value:"Verify Docker GPU Support",id:"verify-docker-gpu-support",level:3},{value:"Enable GPU in Docker Compose",id:"enable-gpu-in-docker-compose",level:3},{value:"Apply GPU Configuration",id:"apply-gpu-configuration",level:3},{value:"Verify GPU is Working",id:"verify-gpu-is-working",level:3},{value:"Performance Comparison",id:"performance-comparison",level:3},{value:"GPU Troubleshooting",id:"gpu-troubleshooting",level:3},{value:"Standalone Setup",id:"standalone-setup",level:2},{value:"Start Standalone Ollama",id:"start-standalone-ollama",level:3},{value:"Configure OpenRegister for Standalone",id:"configure-openregister-for-standalone",level:3},{value:"Using Ollama Features",id:"using-ollama-features",level:2},{value:"1. Chat",id:"1-chat",level:3},{value:"2. RAG (Retrieval Augmented Generation)",id:"2-rag-retrieval-augmented-generation",level:3},{value:"3. Agents with Tools",id:"3-agents-with-tools",level:3},{value:"Managing Models",id:"managing-models",level:2},{value:"List Installed Models",id:"list-installed-models",level:3},{value:"Remove a Model",id:"remove-a-model",level:3},{value:"Update a Model",id:"update-a-model",level:3},{value:"Check Ollama Status",id:"check-ollama-status",level:3},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"Memory Configuration",id:"memory-configuration",level:3},{value:"Concurrent Requests",id:"concurrent-requests",level:3},{value:"Environment Variables",id:"environment-variables",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Connection Error: &quot;Failed to connect to localhost port 11434&quot;",id:"connection-error-failed-to-connect-to-localhost-port-11434",level:3},{value:"Model Not Found (404 Error)",id:"model-not-found-404-error",level:3},{value:"Ollama Container Not Starting",id:"ollama-container-not-starting",level:3},{value:"Out of Memory",id:"out-of-memory",level:3},{value:"Slow Responses",id:"slow-responses",level:3},{value:"Model Recommendations by Use Case",id:"model-recommendations-by-use-case",level:2},{value:"General Customer Support",id:"general-customer-support",level:3},{value:"Technical Documentation",id:"technical-documentation",level:3},{value:"Low Resource Environments",id:"low-resource-environments",level:3},{value:"Maximum Quality",id:"maximum-quality",level:3},{value:"Security Considerations",id:"security-considerations",level:2},{value:"Resources",id:"resources",level:2},{value:"FAQ",id:"faq",level:2}];function c(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"using-ollama-with-openregister",children:"Using Ollama with OpenRegister"})}),"\n",(0,r.jsx)(n.p,{children:"OpenRegister supports Ollama for running local Large Language Models (LLMs) for AI-powered features like chat, agents, and RAG (Retrieval Augmented Generation)."}),"\n",(0,r.jsx)(n.h2,{id:"what-is-ollama",children:"What is Ollama?"}),"\n",(0,r.jsx)(n.p,{children:"Ollama allows you to run open-source LLMs locally on your machine, providing:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Privacy"}),": Your data never leaves your infrastructure"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost-effective"}),": No API costs for inference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Customization"}),": Run any compatible model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Offline capability"}),": Works without internet connection"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,r.jsx)(n.p,{children:"OpenRegister uses LLPhant's native Ollama support, which provides:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Direct API Integration"}),": Uses Ollama's native ",(0,r.jsx)(n.code,{children:"/api/"})," endpoints (not OpenAI-compatible layer)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Better Performance"}),": Optimized for Ollama's specific API structure"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simpler Configuration"}),": No need for API key workarounds"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Full Feature Support"}),": Native support for chat, embeddings, and function calling"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(n.h3,{id:"1-start-ollama-container",children:"1. Start Ollama Container"}),"\n",(0,r.jsx)(n.p,{children:"The Ollama container is included in the docker-compose configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Start all services including Ollama\ndocker-compose up -d\n\n# Or specifically start Ollama\ndocker-compose up -d ollama\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-pull-a-model",children:"2. Pull a Model"}),"\n",(0,r.jsx)(n.p,{children:"Pull one of the supported models:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Llama 3.2 (8B) - Recommended, best balance\ndocker exec openregister-ollama ollama pull llama3.2\n\n# Llama 3.2 (3B) - Lighter alternative\ndocker exec openregister-ollama ollama pull llama3.2:3b\n\n# Mistral (7B) - Fast and efficient\ndocker exec openregister-ollama ollama pull mistral:7b\n\n# Phi-3 Mini (3.8B) - Lightweight option\ndocker exec openregister-ollama ollama pull phi3:mini\n\n# CodeLlama (7B) - Optimized for code\ndocker exec openregister-ollama ollama pull codellama:latest\n"})}),"\n",(0,r.jsx)(n.h3,{id:"3-configure-openregister",children:"3. Configure OpenRegister"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Navigate to ",(0,r.jsx)(n.strong,{children:"Settings"})," \u2192 ",(0,r.jsx)(n.strong,{children:"OpenRegister"})," \u2192 ",(0,r.jsx)(n.strong,{children:"LLM Configuration"})]}),"\n",(0,r.jsxs)(n.li,{children:["Select ",(0,r.jsx)(n.strong,{children:"Ollama"})," as the chat provider"]}),"\n",(0,r.jsx)(n.li,{children:"Configure the settings based on your setup:"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Integrated Setup (docker-compose.yml):"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ollama URL"}),": ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})," \u26a0\ufe0f ",(0,r.jsx)(n.strong,{children:"NOT"})," ",(0,r.jsx)(n.code,{children:"http://localhost:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chat Model"}),": ",(0,r.jsx)(n.code,{children:"llama3.2:latest"})," (or model you pulled with full name including tag)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding Model"}),": ",(0,r.jsx)(n.code,{children:"nomic-embed-text:latest"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why not localhost?"})," OpenRegister runs inside Nextcloud container; use container name instead"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Standalone Setup"})," (using ",(0,r.jsx)(n.code,{children:"--profile ollama"}),"):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Via Docker Network"}),": ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})," (from Nextcloud container)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Via Host IP"}),": ",(0,r.jsx)(n.code,{children:"http://YOUR_HOST_IP:11434"})," (e.g., ",(0,r.jsx)(n.code,{children:"http://192.168.1.100:11434"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Different Host"}),": ",(0,r.jsx)(n.code,{children:"http://your-server-ip:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"NOT"}),": ",(0,r.jsx)(n.code,{children:"http://localhost:11434"})," (won't work from inside container)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chat Model"}),": ",(0,r.jsx)(n.code,{children:"llama3.2:latest"})," (use full name with tag)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding Model"}),": ",(0,r.jsx)(n.code,{children:"nomic-embed-text:latest"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"4-pull-embedding-model-for-rag",children:"4. Pull Embedding Model (for RAG)"}),"\n",(0,r.jsx)(n.p,{children:"If using RAG features, pull an embedding model:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Nomic Embed Text - Recommended for embeddings\ndocker exec openregister-ollama ollama pull nomic-embed-text:latest\n\n# Alternative: all-minilm (smaller, faster)\ndocker exec openregister-ollama ollama pull all-minilm:latest\n"})}),"\n",(0,r.jsx)(n.h2,{id:"configuration-details",children:"Configuration Details"}),"\n",(0,r.jsx)(n.h3,{id:"ollama-service-configuration",children:"Ollama Service Configuration"}),"\n",(0,r.jsxs)(n.p,{children:["The Ollama service is configured in ",(0,r.jsx)(n.code,{children:"docker-compose.yml"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'ollama:\n  image: ollama/ollama:latest\n  container_name: openregister-ollama\n  restart: always\n  ports:\n    - "11434:11434"\n  volumes:\n    - ollama:/root/.ollama\n  environment:\n    - OLLAMA_HOST=0.0.0.0\n    - OLLAMA_NUM_PARALLEL=4\n    - OLLAMA_KEEP_ALIVE=30m\n  deploy:\n    resources:\n      limits:\n        memory: 16G\n      reservations:\n        memory: 8G\n        devices:\n          - driver: nvidia\n            count: all\n            capabilities: [gpu]\n  healthcheck:\n    test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n'})}),"\n",(0,r.jsx)(n.h3,{id:"accessing-ollama",children:"Accessing Ollama"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Important: Docker Container Communication"})}),"\n",(0,r.jsxs)(n.p,{children:["When configuring Ollama in OpenRegister, you MUST use the Docker service name, not ",(0,r.jsx)(n.code,{children:"localhost"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Correct (from Nextcloud container)"}),": ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})," or ",(0,r.jsx)(n.code,{children:"http://ollama:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"Wrong"}),": ",(0,r.jsx)(n.code,{children:"http://localhost:11434"})," (this only works from your host machine, not from inside containers)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Access Points:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"From host machine"})," (terminal, browser): ",(0,r.jsx)(n.code,{children:"http://localhost:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"From Nextcloud container"})," (OpenRegister settings): ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})," or ",(0,r.jsx)(n.code,{children:"http://ollama:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"From other Docker containers"}),": ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why?"})," Inside a Docker container, ",(0,r.jsx)(n.code,{children:"localhost"})," refers to the container itself, not your host machine. Containers communicate with each other using service names defined in docker-compose.yml."]}),"\n",(0,r.jsx)(n.h3,{id:"model-naming",children:"Model Naming"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Important"}),": Ollama models must be referenced with their full name including version tags:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Correct"}),": ",(0,r.jsx)(n.code,{children:"mistral:7b"}),", ",(0,r.jsx)(n.code,{children:"llama3.2:latest"}),", ",(0,r.jsx)(n.code,{children:"phi3:mini"})]}),"\n",(0,r.jsxs)(n.li,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"Wrong"}),": ",(0,r.jsx)(n.code,{children:"mistral"}),", ",(0,r.jsx)(n.code,{children:"llama3.2"}),", ",(0,r.jsx)(n.code,{children:"phi3"})," (without tags)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The OpenRegister UI dropdown shows full model names with tags to ensure compatibility."}),"\n",(0,r.jsx)(n.h2,{id:"recommended-models",children:"Recommended Models"}),"\n",(0,r.jsx)(n.h3,{id:"for-chat--agents",children:"For Chat & Agents"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Size"}),(0,r.jsx)(n.th,{children:"RAM Required"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"Quality"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsxs)(n.td,{children:[(0,r.jsxs)(n.strong,{children:["llama3.2",":latest"]})," \u2b50"]}),(0,r.jsx)(n.td,{children:"4.7GB"}),(0,r.jsx)(n.td,{children:"8-16GB"}),(0,r.jsx)(n.td,{children:"Fast"}),(0,r.jsx)(n.td,{children:"Excellent"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.strong,{children:"RECOMMENDED"})," - Latest, best balance"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"llama3.2:3b"})}),(0,r.jsx)(n.td,{children:"2.0GB"}),(0,r.jsx)(n.td,{children:"4-8GB"}),(0,r.jsx)(n.td,{children:"Very Fast"}),(0,r.jsx)(n.td,{children:"Very Good"}),(0,r.jsx)(n.td,{children:"Lighter, faster alternative"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsxs)(n.strong,{children:["llama3.1",":latest"]})}),(0,r.jsx)(n.td,{children:"4.7GB"}),(0,r.jsx)(n.td,{children:"8-16GB"}),(0,r.jsx)(n.td,{children:"Fast"}),(0,r.jsx)(n.td,{children:"Excellent"}),(0,r.jsx)(n.td,{children:"Previous gen, still great"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.strong,{children:"llama3.2:70b"})," \ud83d\udd25"]}),(0,r.jsx)(n.td,{children:"40GB"}),(0,r.jsx)(n.td,{children:"64-80GB"}),(0,r.jsx)(n.td,{children:"Slow"}),(0,r.jsx)(n.td,{children:"Best"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.strong,{children:"HIGH-END"})," - Maximum quality"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"mistral:7b"})}),(0,r.jsx)(n.td,{children:"4.1GB"}),(0,r.jsx)(n.td,{children:"8GB"}),(0,r.jsx)(n.td,{children:"Very Fast"}),(0,r.jsx)(n.td,{children:"Very Good"}),(0,r.jsx)(n.td,{children:"Fast responses"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsxs)(n.strong,{children:["phi3",":mini"]})}),(0,r.jsx)(n.td,{children:"2.3GB"}),(0,r.jsx)(n.td,{children:"4GB"}),(0,r.jsx)(n.td,{children:"Very Fast"}),(0,r.jsx)(n.td,{children:"Good"}),(0,r.jsx)(n.td,{children:"Low-resource environments"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsxs)(n.strong,{children:["codellama",":latest"]})}),(0,r.jsx)(n.td,{children:"3.8GB"}),(0,r.jsx)(n.td,{children:"8GB"}),(0,r.jsx)(n.td,{children:"Fast"}),(0,r.jsx)(n.td,{children:"Excellent (code)"}),(0,r.jsx)(n.td,{children:"Code generation/analysis"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Memory Configuration Notes:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Our docker-compose files are configured with ",(0,r.jsx)(n.strong,{children:"16GB limit"})," (suitable for 8B models)"]}),"\n",(0,r.jsxs)(n.li,{children:["For 70B models, increase memory limit to ",(0,r.jsx)(n.strong,{children:"80GB"})," in docker-compose"]}),"\n",(0,r.jsxs)(n.li,{children:["Shared memory (",(0,r.jsx)(n.code,{children:"shm_size"}),") set to ",(0,r.jsx)(n.strong,{children:"2GB"})," for efficient model loading"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"for-embeddings-rag",children:"For Embeddings (RAG)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Size"}),(0,r.jsx)(n.th,{children:"Dimensions"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsxs)(n.strong,{children:["nomic-embed-text",":latest"]})}),(0,r.jsx)(n.td,{children:"274MB"}),(0,r.jsx)(n.td,{children:"768"}),(0,r.jsx)(n.td,{children:"Recommended for most uses"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsxs)(n.strong,{children:["all-minilm",":latest"]})}),(0,r.jsx)(n.td,{children:"45MB"}),(0,r.jsx)(n.td,{children:"384"}),(0,r.jsx)(n.td,{children:"Lightweight, faster"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsxs)(n.strong,{children:["mxbai-embed-large",":latest"]})}),(0,r.jsx)(n.td,{children:"670MB"}),(0,r.jsx)(n.td,{children:"1024"}),(0,r.jsx)(n.td,{children:"Highest quality"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"gpu-support",children:"GPU Support"}),"\n",(0,r.jsxs)(n.p,{children:["GPU acceleration provides ",(0,r.jsx)(n.strong,{children:"10-100x speedup"})," for model inference, dramatically improving response times and enabling larger models to run smoothly."]}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA GPU"})," with CUDA support (check compatibility at ",(0,r.jsx)(n.a,{href:"https://developer.nvidia.com/cuda-gpus",children:"https://developer.nvidia.com/cuda-gpus"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA drivers"})," installed on Windows/Linux host"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"WSL2"})," (if using Windows with WSL)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA Container Toolkit"})," for Docker"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"verify-wsl-gpu-support-windows--wsl",children:"Verify WSL GPU Support (Windows + WSL)"}),"\n",(0,r.jsx)(n.p,{children:"First, check if your GPU is accessible from WSL2:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check GPU is visible in WSL\nnvidia-smi\n\n# Should show your GPU, driver version, and CUDA version\n# Example output:\n# NVIDIA-SMI 546.30    Driver Version: 546.30    CUDA Version: 12.3\n# GPU Name: NVIDIA GeForce RTX 3070 Laptop GPU\n"})}),"\n",(0,r.jsx)(n.p,{children:"If 'nvidia-smi' is not found, you need to:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Install latest NVIDIA drivers on Windows host"}),"\n",(0,r.jsxs)(n.li,{children:["Update WSL2 kernel: ",(0,r.jsx)(n.code,{children:"wsl --update"})]}),"\n",(0,r.jsxs)(n.li,{children:["Restart WSL: ",(0,r.jsx)(n.code,{children:"wsl --shutdown"})," then reopen"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"verify-docker-gpu-support",children:"Verify Docker GPU Support"}),"\n",(0,r.jsx)(n.p,{children:"Check if Docker can access the GPU:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Test GPU access from Docker\ndocker run --rm --gpus all nvidia/cuda:12.3.0-base-ubuntu20.04 nvidia-smi\n\n# Should show the same GPU information\n# If this fails, install NVIDIA Container Toolkit:\n# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html\n"})}),"\n",(0,r.jsx)(n.h3,{id:"enable-gpu-in-docker-compose",children:"Enable GPU in Docker Compose"}),"\n",(0,r.jsx)(n.p,{children:"The GPU configuration is already included in docker-compose files. Verify it contains:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"ollama:\n  image: ollama/ollama:latest\n  deploy:\n    resources:\n      reservations:\n        devices:\n          - driver: nvidia\n            count: all  # Use all available GPUs (or count: 1 for single GPU)\n            capabilities: [gpu]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"apply-gpu-configuration",children:"Apply GPU Configuration"}),"\n",(0,r.jsx)(n.p,{children:"If using docker-compose:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Stop and remove the old container\ndocker stop openregister-ollama\ndocker rm openregister-ollama\n\n# Recreate with GPU support\ndocker-compose up -d ollama\n"})}),"\n",(0,r.jsx)(n.p,{children:"If using docker run (alternative method):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Stop old container\ndocker stop openregister-ollama\ndocker rm openregister-ollama\n\n# Start with GPU support\ndocker run -d \\\n  --name openregister-ollama \\\n  --restart always \\\n  -p 11434:11434 \\\n  -v openregister_ollama:/root/.ollama \\\n  -e OLLAMA_HOST=0.0.0.0 \\\n  -e OLLAMA_NUM_PARALLEL=4 \\\n  -e OLLAMA_KEEP_ALIVE=30m \\\n  --shm-size=2gb \\\n  --gpus all \\\n  ollama/ollama:latest\n"})}),"\n",(0,r.jsx)(n.h3,{id:"verify-gpu-is-working",children:"Verify GPU is Working"}),"\n",(0,r.jsx)(n.p,{children:"After restarting Ollama with GPU support:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# 1. Check GPU is accessible inside container\ndocker exec openregister-ollama nvidia-smi\n\n# Should show GPU information from inside the container\n\n# 2. Check Docker device configuration\ndocker inspect openregister-ollama --format \'{{json .HostConfig.DeviceRequests}}\'\n\n# Should show: [{"Driver":"","Count":-1,"Capabilities":[["gpu"]],"Options":{}}]\n\n# 3. Check Ollama logs for GPU detection\ndocker logs openregister-ollama | grep -i gpu\n\n# Should show lines like:\n# inference compute id=GPU-xxx library=CUDA compute=8.6 name=CUDA0\n# description="NVIDIA GeForce RTX 3070 Laptop GPU"\n# total="8.0 GiB" available="6.2 GiB"\n\n# 4. Test inference speed (should be much faster)\ndocker exec openregister-ollama ollama run llama3.2:latest "What is 2+2?"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"performance-comparison",children:"Performance Comparison"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Mode"}),(0,r.jsx)(n.th,{children:"Loading Time"}),(0,r.jsx)(n.th,{children:"First Token"}),(0,r.jsx)(n.th,{children:"Tokens/sec"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"CPU"})}),(0,r.jsx)(n.td,{children:"30-60s"}),(0,r.jsx)(n.td,{children:"5-10s"}),(0,r.jsx)(n.td,{children:"2-5"}),(0,r.jsx)(n.td,{children:"Testing only"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"GPU"})}),(0,r.jsx)(n.td,{children:"2-5s"}),(0,r.jsx)(n.td,{children:"0.5-1s"}),(0,r.jsx)(n.td,{children:"50-200"}),(0,r.jsx)(n.td,{children:"Production use"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"gpu-troubleshooting",children:"GPU Troubleshooting"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Issue: 'nvidia-smi: command not found' in container"})}),"\n",(0,r.jsx)(n.p,{children:"This means GPU is NOT configured. Check:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Verify DeviceRequests is not null\ndocker inspect openregister-ollama --format '{{json .HostConfig.DeviceRequests}}'\n\n# If null, container was created without GPU support\n# Solution: Remove and recreate with --gpus flag or proper docker-compose config\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Issue: GPU not detected in Ollama logs"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Check Ollama startup logs\ndocker logs openregister-ollama 2>&1 | grep -A 5 "discovering available GPUs"\n\n# If no GPU found, check:\n# 1. NVIDIA drivers installed on host\n# 2. nvidia-smi works on host\n# 3. Docker GPU support working (test with nvidia/cuda image)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Issue: 'Failed to initialize NVML: Unknown Error'"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# This usually means driver mismatch\n# Solution: Update NVIDIA drivers on host machine\n# Then restart Docker and WSL:\nwsl --shutdown\n# Restart Docker Desktop\n# Recreate Ollama container\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Issue: Out of VRAM"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check GPU memory usage\nnvidia-smi\n\n# If VRAM is full:\n# 1. Use smaller model (3B instead of 8B)\n# 2. Reduce OLLAMA_MAX_LOADED_MODELS\n# 3. Set shorter OLLAMA_KEEP_ALIVE (e.g., 5m)\n# 4. Restart container to clear VRAM\ndocker restart openregister-ollama\n"})}),"\n",(0,r.jsx)(n.h2,{id:"standalone-setup",children:"Standalone Setup"}),"\n",(0,r.jsx)(n.p,{children:"Running Ollama standalone is ideal for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udda5\ufe0f ",(0,r.jsx)(n.strong,{children:"Separate Hardware"}),": Run on a more powerful machine with more RAM/GPU"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd27 ",(0,r.jsx)(n.strong,{children:"Production"}),": Isolate AI workloads from application stack"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Scalability"}),": Serve multiple OpenRegister instances from one Ollama"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcb0 ",(0,r.jsx)(n.strong,{children:"Cost Optimization"}),": Use a dedicated GPU machine only for AI"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"start-standalone-ollama",children:"Start Standalone Ollama"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# From the openregister directory (using ollama profile)\ndocker-compose -f docker-compose.dev.yml --profile ollama up -d\n\n# Check status\ndocker-compose -f docker-compose.dev.yml --profile ollama ps\n\n# View logs\ndocker-compose -f docker-compose.dev.yml --profile ollama logs -f ollama\n"})}),"\n",(0,r.jsx)(n.h3,{id:"configure-openregister-for-standalone",children:"Configure OpenRegister for Standalone"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Option 1: Use Host IP Address (Simplest)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["URL: ",(0,r.jsx)(n.code,{children:"http://YOUR_HOST_IP:11434"})," (e.g., ",(0,r.jsx)(n.code,{children:"http://192.168.1.100:11434"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:["Model: ",(0,r.jsx)(n.code,{children:"llama3.2:latest"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Find your IP"}),": Run ",(0,r.jsx)(n.code,{children:"hostname -I | awk '{print $1}'"})," on Linux/Mac or ",(0,r.jsx)(n.code,{children:"ipconfig"})," on Windows"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Option 2: Connect via Docker Network (Recommended for Production)"})}),"\n",(0,r.jsx)(n.p,{children:"First, connect the standalone Ollama container to your Nextcloud network:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Find your Nextcloud network\ndocker network ls | grep master\n\n# Connect standalone Ollama to that network\ndocker network connect master_default standalone-ollama\n\n# Verify connection from Nextcloud container\ndocker exec master-nextcloud-1 curl -s http://standalone-ollama:11434/api/tags\n"})}),"\n",(0,r.jsx)(n.p,{children:"Then in OpenRegister settings:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["URL: ",(0,r.jsx)(n.code,{children:"http://standalone-ollama:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:["Model: ",(0,r.jsx)(n.code,{children:"llama3.2:latest"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Option 3: Different Physical Machine"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["URL: ",(0,r.jsx)(n.code,{children:"http://your-ollama-server-ip:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:["Model: ",(0,r.jsx)(n.code,{children:"llama3.2:latest"})]}),"\n",(0,r.jsx)(n.li,{children:"Ensure port 11434 is accessible (firewall rules)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"using-ollama-features",children:"Using Ollama Features"}),"\n",(0,r.jsx)(n.h3,{id:"1-chat",children:"1. Chat"}),"\n",(0,r.jsx)(n.p,{children:"Once configured, the chat feature works automatically:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Go to any page in OpenRegister"}),"\n",(0,r.jsx)(n.li,{children:"Click the chat icon in the sidebar"}),"\n",(0,r.jsx)(n.li,{children:"Start a conversation with your local AI"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-rag-retrieval-augmented-generation",children:"2. RAG (Retrieval Augmented Generation)"}),"\n",(0,r.jsx)(n.p,{children:"Enable RAG for agents to search your documents:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create/edit an agent"}),"\n",(0,r.jsxs)(n.li,{children:["Enable ",(0,r.jsx)(n.strong,{children:"RAG"})," in agent settings"]}),"\n",(0,r.jsxs)(n.li,{children:["Configure:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Search Mode"}),": Vector, Hybrid, or Full-text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Number of Sources"}),": How many documents to retrieve"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Include Files"}),": Search uploaded files"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Include Objects"}),": Search OpenRegister objects"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-agents-with-tools",children:"3. Agents with Tools"}),"\n",(0,r.jsx)(n.p,{children:"Create agents that can interact with your data:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create a new agent"}),"\n",(0,r.jsxs)(n.li,{children:["Enable tools:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Register Tool"}),": Query registers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Schema Tool"}),": Access schemas"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Objects Tool"}),": Manipulate objects"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Set the system prompt"}),"\n",(0,r.jsx)(n.li,{children:"Chat with the agent"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"managing-models",children:"Managing Models"}),"\n",(0,r.jsx)(n.h3,{id:"list-installed-models",children:"List Installed Models"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker exec openregister-ollama ollama list\n"})}),"\n",(0,r.jsx)(n.h3,{id:"remove-a-model",children:"Remove a Model"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker exec openregister-ollama ollama rm llama2\n"})}),"\n",(0,r.jsx)(n.h3,{id:"update-a-model",children:"Update a Model"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker exec openregister-ollama ollama pull llama3.2:latest\n"})}),"\n",(0,r.jsx)(n.h3,{id:"check-ollama-status",children:"Check Ollama Status"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check if Ollama is running\ncurl http://localhost:11434/api/tags\n\n# View running models\ndocker exec openregister-ollama ollama ps\n"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,r.jsx)(n.h3,{id:"memory-configuration",children:"Memory Configuration"}),"\n",(0,r.jsx)(n.p,{children:"Adjust model context size in OpenRegister settings:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Max Tokens"}),": Control response length (default: 2048)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Temperature"}),": Control randomness (0.0-1.0, default: 0.7)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"concurrent-requests",children:"Concurrent Requests"}),"\n",(0,r.jsx)(n.p,{children:"Ollama can handle multiple concurrent requests. Monitor resource usage:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check container resource usage\ndocker stats openregister-ollama\n"})}),"\n",(0,r.jsx)(n.h3,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,r.jsxs)(n.p,{children:["Edit ",(0,r.jsx)(n.code,{children:"docker-compose.yml"})," to tune performance:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"environment:\n  # Number of concurrent requests (default: 4)\n  - OLLAMA_NUM_PARALLEL=8\n  \n  # How long to keep models in memory (default: 30m)\n  - OLLAMA_KEEP_ALIVE=1h\n  \n  # Maximum models to keep loaded (default: 2)\n  - OLLAMA_MAX_LOADED_MODELS=3\n"})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"connection-error-failed-to-connect-to-localhost-port-11434",children:'Connection Error: "Failed to connect to localhost port 11434"'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Error: "cURL error 7: Failed to connect to localhost port 11434"'}),"\n",(0,r.jsx)(n.li,{children:'Error: "Could not connect to server"'}),"\n",(0,r.jsx)(n.li,{children:"Chat test fails with connection refused"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cause:"})," You're using ",(0,r.jsx)(n.code,{children:"http://localhost:11434"})," in OpenRegister settings, but inside the Nextcloud container, ",(0,r.jsx)(n.code,{children:"localhost"})," refers to the container itself, not the host machine."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Update Ollama URL in OpenRegister settings:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Go to ",(0,r.jsx)(n.strong,{children:"Settings"})," \u2192 ",(0,r.jsx)(n.strong,{children:"OpenRegister"})," \u2192 ",(0,r.jsx)(n.strong,{children:"LLM Configuration"})]}),"\n",(0,r.jsxs)(n.li,{children:["Change Ollama URL from ",(0,r.jsx)(n.code,{children:"http://localhost:11434"})," to ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:["Click ",(0,r.jsx)(n.strong,{children:"Save"})," and ",(0,r.jsx)(n.strong,{children:"Test Connection"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Verify containers are on the same network:"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check if Nextcloud can reach Ollama\ndocker exec master-nextcloud-1 curl -s http://openregister-ollama:11434/api/tags\n\n# Should return JSON with model list\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"If still failing, connect containers to the same network:"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Find your Nextcloud network\ndocker network ls | grep master\n\n# Connect Ollama to the same network\ndocker network connect master_default openregister-ollama\n\n# Test again\ndocker exec master-nextcloud-1 curl -s http://openregister-ollama:11434/api/tags\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Remember:"})," Always use the ",(0,r.jsx)(n.strong,{children:"container/service name"}),", not ",(0,r.jsx)(n.code,{children:"localhost"}),", when configuring services inside Docker."]}),"\n",(0,r.jsx)(n.h3,{id:"model-not-found-404-error",children:"Model Not Found (404 Error)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cause:"})," Model name doesn't include version tag."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution:"})," Use full model name with tag:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.code,{children:"mistral:7b"})," (correct)"]}),"\n",(0,r.jsxs)(n.li,{children:["\u274c ",(0,r.jsx)(n.code,{children:"mistral"})," (wrong - missing tag)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Update model name in OpenRegister settings to include the tag."}),"\n",(0,r.jsx)(n.h3,{id:"ollama-container-not-starting",children:"Ollama Container Not Starting"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check logs\ndocker logs openregister-ollama\n\n# Restart the container\ndocker-compose restart ollama\n"})}),"\n",(0,r.jsx)(n.h3,{id:"out-of-memory",children:"Out of Memory"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Use a smaller model (phi3",":mini",", mistral:7b)"]}),"\n",(0,r.jsx)(n.li,{children:"Reduce max tokens in settings"}),"\n",(0,r.jsx)(n.li,{children:"Allocate more RAM to Docker"}),"\n",(0,r.jsx)(n.li,{children:"Stop other services temporarily"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"slow-responses",children:"Slow Responses"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Use a faster model (mistral:7b, phi3",":mini",")"]}),"\n",(0,r.jsx)(n.li,{children:"Reduce context window"}),"\n",(0,r.jsx)(n.li,{children:"Enable GPU support"}),"\n",(0,r.jsxs)(n.li,{children:["Check system resources: ",(0,r.jsx)(n.code,{children:"docker stats"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"model-recommendations-by-use-case",children:"Model Recommendations by Use Case"}),"\n",(0,r.jsx)(n.h3,{id:"general-customer-support",children:"General Customer Support"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": llama3.2",":latest"," or mistral:7b"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Good balance of quality and speed"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"technical-documentation",children:"Technical Documentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": codellama",":latest"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Better understanding of technical content"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"low-resource-environments",children:"Low Resource Environments"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": phi3",":mini"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Smallest footprint while maintaining quality"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"maximum-quality",children:"Maximum Quality"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": llama3.2:70b (requires 64GB+ RAM)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Best possible responses (if you have the resources)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Network Isolation"}),": Ollama is only accessible within the Docker network by default"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No External API Calls"}),": All processing happens locally"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Privacy"}),": Your data never leaves your infrastructure"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Safety"}),": Use official models from Ollama library"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ollama Documentation"}),": ",(0,r.jsx)(n.a,{href:"https://ollama.ai/",children:"https://ollama.ai/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Library"}),": ",(0,r.jsx)(n.a,{href:"https://ollama.ai/library",children:"https://ollama.ai/library"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GitHub"}),": ",(0,r.jsx)(n.a,{href:"https://github.com/ollama/ollama",children:"https://github.com/ollama/ollama"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenRegister AI Docs"}),": See ",(0,r.jsx)(n.a,{href:"/docs/Features/ai",children:"AI Features"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"faq",children:"FAQ"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Q: Can I use multiple models simultaneously?"}),(0,r.jsx)(n.br,{}),"\n","A: Yes, configure different models for chat vs embeddings."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Q: How much disk space do I need?"}),(0,r.jsx)(n.br,{}),"\n","A: Plan for 5-10GB per model. Embedding models are smaller (~500MB)."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Q: Can I use custom models?"}),(0,r.jsx)(n.br,{}),"\n","A: Yes, see Ollama documentation on creating custom Modelfiles."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Q: Does it work on ARM (Apple Silicon)?"}),(0,r.jsx)(n.br,{}),"\n","A: Yes, Ollama supports ARM64 architecture."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Q: Can I use Ollama with other OpenRegister apps?"}),(0,r.jsx)(n.br,{}),"\n","A: Yes, any app in the workspace can access the Ollama container."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);