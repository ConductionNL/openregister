"use strict";(self.webpackChunkopen_catalogi_docs=self.webpackChunkopen_catalogi_docs||[]).push([[3458],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var r=i(96540);const l={},s=r.createContext(l);function t(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:t(e.components),r.createElement(s.Provider,{value:n},e.children)}},48134:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"Integrations/mistral","title":"Mistral LLM Model Integration","description":"Mistral is a high-performance open-source language model that can be used with OpenRegister through Ollama or Hugging Face integrations.","source":"@site/docs/Integrations/mistral.md","sourceDirName":"Integrations","slug":"/Integrations/mistral","permalink":"/docs/Integrations/mistral","draft":false,"unlisted":false,"editUrl":"https://github.com/conductionnl/openregister/tree/main/website/docs/Integrations/mistral.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Hugging Face Integration","permalink":"/docs/Integrations/huggingface"},"next":{"title":"n8n Integration","permalink":"/docs/Integrations/n8n"}}');var l=i(74848),s=i(28453);const t={},a="Mistral LLM Model Integration",d={},o=[{value:"Overview",id:"overview",level:2},{value:"Model Variants",id:"model-variants",level:2},{value:"Using Mistral with Ollama",id:"using-mistral-with-ollama",level:2},{value:"Quick Start",id:"quick-start",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Using Mistral with Hugging Face",id:"using-mistral-with-hugging-face",level:2},{value:"Quick Start",id:"quick-start-1",level:3},{value:"Configuration",id:"configuration-1",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"1. General Purpose Chat",id:"1-general-purpose-chat",level:3},{value:"2. RAG (Retrieval Augmented Generation)",id:"2-rag-retrieval-augmented-generation",level:3},{value:"3. Function Calling",id:"3-function-calling",level:3},{value:"Performance Comparison",id:"performance-comparison",level:2},{value:"Recommended Configuration",id:"recommended-configuration",level:2},{value:"For Development",id:"for-development",level:3},{value:"For Production",id:"for-production",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Model Not Found (Ollama)",id:"model-not-found-ollama",level:3},{value:"Slow Performance",id:"slow-performance",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Support",id:"support",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"mistral-llm-model-integration",children:"Mistral LLM Model Integration"})}),"\n",(0,l.jsx)(n.p,{children:"Mistral is a high-performance open-source language model that can be used with OpenRegister through Ollama or Hugging Face integrations."}),"\n",(0,l.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,l.jsx)(n.p,{children:"Mistral models are available in multiple sizes and can be run locally using:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Ollama"}),": Simple setup, native API"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hugging Face TGI/vLLM"}),": OpenAI-compatible API, optimized for production"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"model-variants",children:"Model Variants"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Model"}),(0,l.jsx)(n.th,{children:"Size"}),(0,l.jsx)(n.th,{children:"Parameters"}),(0,l.jsx)(n.th,{children:"Use Case"}),(0,l.jsx)(n.th,{children:"Memory Required"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Mistral 7B"})}),(0,l.jsx)(n.td,{children:"7B"}),(0,l.jsx)(n.td,{children:"7 billion"}),(0,l.jsx)(n.td,{children:"General purpose, RAG"}),(0,l.jsx)(n.td,{children:"16GB"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Mistral 7B Instruct"})}),(0,l.jsx)(n.td,{children:"7B"}),(0,l.jsx)(n.td,{children:"7 billion"}),(0,l.jsx)(n.td,{children:"Chat, instructions"}),(0,l.jsx)(n.td,{children:"16GB"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Mixtral 8x7B"})}),(0,l.jsx)(n.td,{children:"47B"}),(0,l.jsx)(n.td,{children:"47 billion"}),(0,l.jsx)(n.td,{children:"High quality, complex tasks"}),(0,l.jsx)(n.td,{children:"48GB+"})]})]})]}),"\n",(0,l.jsx)(n.h2,{id:"using-mistral-with-ollama",children:"Using Mistral with Ollama"}),"\n",(0,l.jsx)(n.h3,{id:"quick-start",children:"Quick Start"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"# Pull Mistral model\ndocker exec openregister-ollama ollama pull mistral:7b\n\n# Or Mistral Instruct (recommended for chat)\ndocker exec openregister-ollama ollama pull mistral:latest\n"})}),"\n",(0,l.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Navigate to ",(0,l.jsx)(n.strong,{children:"Settings"})," \u2192 ",(0,l.jsx)(n.strong,{children:"OpenRegister"})," \u2192 ",(0,l.jsx)(n.strong,{children:"LLM Configuration"})]}),"\n",(0,l.jsxs)(n.li,{children:["Select ",(0,l.jsx)(n.strong,{children:"Ollama"})," as provider"]}),"\n",(0,l.jsxs)(n.li,{children:["Configure:","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Ollama URL"}),": ",(0,l.jsx)(n.code,{children:"http://openregister-ollama:11434"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Chat Model"}),": ",(0,l.jsx)(n.code,{children:"mistral:latest"})," or ",(0,l.jsx)(n.code,{children:"mistral:7b"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["See ",(0,l.jsx)(n.a,{href:"/docs/Integrations/ollama",children:"Ollama Integration"})," for detailed setup instructions."]}),"\n",(0,l.jsx)(n.h2,{id:"using-mistral-with-hugging-face",children:"Using Mistral with Hugging Face"}),"\n",(0,l.jsx)(n.h3,{id:"quick-start-1",children:"Quick Start"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"# Start TGI with Mistral (using huggingface profile)\ndocker-compose -f docker-compose.dev.yml --profile huggingface up -d tgi-mistral\n\n# Or start vLLM with Mistral (if configured)\ndocker-compose -f docker-compose.dev.yml --profile huggingface up -d vllm-mistral\n"})}),"\n",(0,l.jsx)(n.h3,{id:"configuration-1",children:"Configuration"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Navigate to ",(0,l.jsx)(n.strong,{children:"Settings"})," \u2192 ",(0,l.jsx)(n.strong,{children:"OpenRegister"})," \u2192 ",(0,l.jsx)(n.strong,{children:"LLM Configuration"})]}),"\n",(0,l.jsxs)(n.li,{children:["Select ",(0,l.jsx)(n.strong,{children:"OpenAI"})," as provider (TGI/vLLM are OpenAI-compatible)"]}),"\n",(0,l.jsxs)(n.li,{children:["Configure:","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Base URL"}),": ",(0,l.jsx)(n.code,{children:"http://tgi-mistral:80"})," (TGI) or ",(0,l.jsx)(n.code,{children:"http://vllm-mistral:8000"})," (vLLM)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Model"}),": ",(0,l.jsx)(n.code,{children:"mistral-7b-instruct"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"API Key"}),": ",(0,l.jsx)(n.code,{children:"dummy"})," (not used for local)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["See ",(0,l.jsx)(n.a,{href:"/docs/Integrations/huggingface",children:"Hugging Face Integration"})," for detailed setup instructions."]}),"\n",(0,l.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,l.jsx)(n.h3,{id:"1-general-purpose-chat",children:"1. General Purpose Chat"}),"\n",(0,l.jsx)(n.p,{children:"Mistral excels at:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Conversational AI"}),"\n",(0,l.jsx)(n.li,{children:"Question answering"}),"\n",(0,l.jsx)(n.li,{children:"Text generation"}),"\n",(0,l.jsx)(n.li,{children:"Code generation"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"2-rag-retrieval-augmented-generation",children:"2. RAG (Retrieval Augmented Generation)"}),"\n",(0,l.jsx)(n.p,{children:"Use Mistral with OpenRegister's RAG features:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Answer questions using your data"}),"\n",(0,l.jsx)(n.li,{children:"Context-aware responses"}),"\n",(0,l.jsx)(n.li,{children:"Citation support"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"3-function-calling",children:"3. Function Calling"}),"\n",(0,l.jsx)(n.p,{children:"Mistral supports function calling for:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Object search"}),"\n",(0,l.jsx)(n.li,{children:"Object creation"}),"\n",(0,l.jsx)(n.li,{children:"Object updates"}),"\n",(0,l.jsx)(n.li,{children:"Register queries"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"performance-comparison",children:"Performance Comparison"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Setup"}),(0,l.jsx)(n.th,{children:"Speed"}),(0,l.jsx)(n.th,{children:"Quality"}),(0,l.jsx)(n.th,{children:"Ease of Use"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Ollama"})}),(0,l.jsx)(n.td,{children:"\u26a1\u26a1\u26a1 Fast"}),(0,l.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50"}),(0,l.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50\u2b50 Easy"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"TGI"})}),(0,l.jsx)(n.td,{children:"\u26a1\u26a1 Fast"}),(0,l.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50"}),(0,l.jsx)(n.td,{children:"\u2b50\u2b50\u2b50 Medium"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"vLLM"})}),(0,l.jsx)(n.td,{children:"\u26a1\u26a1\u26a1 Very Fast"}),(0,l.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50"}),(0,l.jsx)(n.td,{children:"\u2b50\u2b50\u2b50 Medium"})]})]})]}),"\n",(0,l.jsx)(n.h2,{id:"recommended-configuration",children:"Recommended Configuration"}),"\n",(0,l.jsx)(n.h3,{id:"for-development",children:"For Development"}),"\n",(0,l.jsxs)(n.p,{children:["Use ",(0,l.jsx)(n.strong,{children:"Ollama"})," with Mistral:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Easiest setup"}),"\n",(0,l.jsx)(n.li,{children:"Good performance"}),"\n",(0,l.jsx)(n.li,{children:"Native API"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"for-production",children:"For Production"}),"\n",(0,l.jsxs)(n.p,{children:["Use ",(0,l.jsx)(n.strong,{children:"TGI"})," or ",(0,l.jsx)(n.strong,{children:"vLLM"})," with Mistral:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Better throughput"}),"\n",(0,l.jsx)(n.li,{children:"OpenAI-compatible API"}),"\n",(0,l.jsx)(n.li,{children:"Optimized inference"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,l.jsx)(n.h3,{id:"model-not-found-ollama",children:"Model Not Found (Ollama)"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"# List available models\ndocker exec openregister-ollama ollama list\n\n# Pull Mistral if missing\ndocker exec openregister-ollama ollama pull mistral:latest\n\n# Verify model name includes tag\ndocker exec openregister-ollama ollama show mistral:latest\n"})}),"\n",(0,l.jsx)(n.h3,{id:"slow-performance",children:"Slow Performance"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Use GPU acceleration (10-100x faster)"}),"\n",(0,l.jsx)(n.li,{children:"Use Mistral 7B instead of Mixtral 8x7B"}),"\n",(0,l.jsx)(n.li,{children:"Ensure models are loaded in memory"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"/docs/Integrations/ollama",children:"Ollama Integration"})," - Run Mistral via Ollama"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"/docs/Integrations/huggingface",children:"Hugging Face Integration"})," - Run Mistral via TGI/vLLM"]}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://mistral.ai",children:"Mistral AI Documentation"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"/docs/features/rag-implementation",children:"RAG Implementation"})}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"support",children:"Support"}),"\n",(0,l.jsx)(n.p,{children:"For issues specific to:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Mistral models"}),": Check ",(0,l.jsx)(n.a,{href:"https://mistral.ai",children:"Mistral AI Documentation"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Ollama setup"}),": See ",(0,l.jsx)(n.a,{href:"/docs/Integrations/ollama",children:"Ollama Integration"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hugging Face setup"}),": See ",(0,l.jsx)(n.a,{href:"/docs/Integrations/huggingface",children:"Hugging Face Integration"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"OpenRegister integration"}),": OpenRegister GitHub issues"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}}}]);