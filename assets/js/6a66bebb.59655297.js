"use strict";(self.webpackChunkopen_catalogi_docs=self.webpackChunkopen_catalogi_docs||[]).push([[1679],{28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>c});var l=i(96540);const t={},s=l.createContext(t);function r(e){const n=l.useContext(s);return l.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),l.createElement(s.Provider,{value:n},e.children)}},65893:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"development/huggingface-tgi-setup","title":"Hugging Face TGI/vLLM Setup - OpenAI-Compatible API","description":"Overview","source":"@site/docs/development/huggingface-tgi-setup.md","sourceDirName":"development","slug":"/development/huggingface-tgi-setup","permalink":"/docs/development/huggingface-tgi-setup","draft":false,"unlisted":false,"editUrl":"https://github.com/conductionnl/openregister/tree/main/website/docs/development/huggingface-tgi-setup.md","tags":[],"version":"current","sidebarPosition":14,"frontMatter":{"title":"Hugging Face TGI/vLLM Setup - OpenAI-Compatible API","sidebar_position":14},"sidebar":"tutorialSidebar","previous":{"title":"Dolphin Document Parser Deployment (Optional)","permalink":"/docs/development/dolphin-deployment"},"next":{"title":"LLPhant Installation Guide","permalink":"/docs/development/llphant-setup"}}');var t=i(74848),s=i(28453);const r={title:"Hugging Face TGI/vLLM Setup - OpenAI-Compatible API",sidebar_position:14},c="Hugging Face Text Generation Inference (TGI) & vLLM Setup",o={},d=[{value:"Overview",id:"overview",level:2},{value:"Why Use TGI or vLLM?",id:"why-use-tgi-or-vllm",level:2},{value:"Benefits",id:"benefits",level:3},{value:"Comparison",id:"comparison",level:3},{value:"Two Options",id:"two-options",level:2},{value:"Option 1: Text Generation Inference (TGI) - Recommended",id:"option-1-text-generation-inference-tgi---recommended",level:3},{value:"Option 2: vLLM - Alternative",id:"option-2-vllm---alternative",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"1. Start TGI with Mistral",id:"1-start-tgi-with-mistral",level:3},{value:"2. Wait for Model Download",id:"2-wait-for-model-download",level:3},{value:"3. Test the API",id:"3-test-the-api",level:3},{value:"Integration with OpenRegister",id:"integration-with-openregister",level:2},{value:"Using with LLPhant (PHP)",id:"using-with-llphant-php",level:3},{value:"Configuration in OpenRegister",id:"configuration-in-openregister",level:3},{value:"Direct API Calls (without LLPhant)",id:"direct-api-calls-without-llphant",level:3},{value:"Available Models",id:"available-models",level:2},{value:"Recommended Models for OpenRegister",id:"recommended-models-for-openregister",level:3},{value:"Changing the Model",id:"changing-the-model",level:3},{value:"OpenAI API Compatibility",id:"openai-api-compatibility",level:2},{value:"Supported Endpoints",id:"supported-endpoints",level:3},{value:"Compatibility Notes",id:"compatibility-notes",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Memory Management",id:"memory-management",level:3},{value:"Batch Processing",id:"batch-processing",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Model Download Fails",id:"model-download-fails",level:3},{value:"Out of Memory",id:"out-of-memory",level:3},{value:"Slow Inference",id:"slow-inference",level:3},{value:"API Connection Refused",id:"api-connection-refused",level:3},{value:"Comparison with Ollama",id:"comparison-with-ollama",level:2},{value:"Integration Examples",id:"integration-examples",level:2},{value:"Example 1: RAG with Local Mistral",id:"example-1-rag-with-local-mistral",level:3},{value:"Example 2: Entity Extraction with Local Mistral",id:"example-2-entity-extraction-with-local-mistral",level:3},{value:"Example 3: Document Summarization",id:"example-3-document-summarization",level:3},{value:"Production Deployment",id:"production-deployment",level:2},{value:"Resource Allocation",id:"resource-allocation",level:3},{value:"Monitoring",id:"monitoring",level:3},{value:"Load Balancing",id:"load-balancing",level:3},{value:"Related Documentation",id:"related-documentation",level:2},{value:"External Resources",id:"external-resources",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"hugging-face-text-generation-inference-tgi--vllm-setup",children:"Hugging Face Text Generation Inference (TGI) & vLLM Setup"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:["Run ",(0,t.jsx)(n.strong,{children:"Mistral"})," and other Hugging Face models locally with an ",(0,t.jsx)(n.strong,{children:"OpenAI-compatible API"}),". This allows OpenRegister's LLPhant library and other OpenAI clients to use local models instead of cloud APIs."]}),"\n",(0,t.jsx)(n.h2,{id:"why-use-tgi-or-vllm",children:"Why Use TGI or vLLM?"}),"\n",(0,t.jsx)(n.h3,{id:"benefits",children:"Benefits"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"OpenAI-Compatible API"})," - Drop-in replacement for OpenAI"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Privacy-First"})," - All data stays local"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Cost-Free"})," - No API fees"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Fast"})," - Optimized inference engines"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Flexible"})," - Choose any Hugging Face model"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"comparison",children:"Comparison"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Feature"}),(0,t.jsx)(n.th,{children:"TGI"}),(0,t.jsx)(n.th,{children:"vLLM"}),(0,t.jsx)(n.th,{children:"Ollama"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"OpenAI API"})}),(0,t.jsx)(n.td,{children:"\u2705 Yes (v1.4.0+)"}),(0,t.jsx)(n.td,{children:"\u2705 Yes"}),(0,t.jsx)(n.td,{children:"\u274c No"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Speed"})}),(0,t.jsx)(n.td,{children:"\u26a1\u26a1 Fast"}),(0,t.jsx)(n.td,{children:"\u26a1\u26a1\u26a1 Very Fast"}),(0,t.jsx)(n.td,{children:"\u26a1 Good"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Models"})}),(0,t.jsx)(n.td,{children:"Hugging Face"}),(0,t.jsx)(n.td,{children:"Hugging Face"}),(0,t.jsx)(n.td,{children:"Curated list"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Setup"})}),(0,t.jsx)(n.td,{children:"Medium"}),(0,t.jsx)(n.td,{children:"Medium"}),(0,t.jsx)(n.td,{children:"Easy"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Memory"})}),(0,t.jsx)(n.td,{children:"8-16GB"}),(0,t.jsx)(n.td,{children:"8-16GB"}),(0,t.jsx)(n.td,{children:"8-16GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Use Case"})}),(0,t.jsx)(n.td,{children:"Production"}),(0,t.jsx)(n.td,{children:"High throughput"}),(0,t.jsx)(n.td,{children:"Simple setup"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"two-options",children:"Two Options"}),"\n",(0,t.jsx)(n.h3,{id:"option-1-text-generation-inference-tgi---recommended",children:"Option 1: Text Generation Inference (TGI) - Recommended"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pros"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Official Hugging Face solution"}),"\n",(0,t.jsx)(n.li,{children:"Well-maintained and documented"}),"\n",(0,t.jsx)(n.li,{children:"Optimized for production"}),"\n",(0,t.jsx)(n.li,{children:"Automatic quantization"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Installation"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Already configured in docker-compose.huggingface.yml\ndocker-compose -f docker-compose.huggingface.yml up -d tgi-mistral\n"})}),"\n",(0,t.jsx)(n.h3,{id:"option-2-vllm---alternative",children:"Option 2: vLLM - Alternative"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pros"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Faster inference"}),"\n",(0,t.jsx)(n.li,{children:"Better throughput for multiple requests"}),"\n",(0,t.jsx)(n.li,{children:"Full OpenAI API compatibility"}),"\n",(0,t.jsx)(n.li,{children:"PagedAttention optimization"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Installation"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Already configured in docker-compose.huggingface.yml\ndocker-compose -f docker-compose.huggingface.yml up -d vllm-mistral\n"})}),"\n",(0,t.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,t.jsx)(n.h3,{id:"1-start-tgi-with-mistral",children:"1. Start TGI with Mistral"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd /path/to/openregister\n\n# Start TGI (choose ONE)\ndocker-compose -f docker-compose.huggingface.yml up -d tgi-mistral\n\n# OR start vLLM\ndocker-compose -f docker-compose.huggingface.yml up -d vllm-mistral\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-wait-for-model-download",children:"2. Wait for Model Download"}),"\n",(0,t.jsx)(n.p,{children:"First startup downloads the model (~15GB for Mistral 7B):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Watch logs\ndocker logs -f openregister-tgi-mistral\n\n# You'll see:\n# Downloading model...\n# Model loaded successfully\n# Serving on port 80\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Time"}),": 5-10 minutes depending on internet speed"]}),"\n",(0,t.jsx)(n.h3,{id:"3-test-the-api",children:"3. Test the API"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"TGI (port 8081)"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl http://localhost:8081/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "mistral-7b-instruct",\n    "messages": [\n      {"role": "user", "content": "Hello! What is the capital of France?"}\n    ],\n    "max_tokens": 100\n  }\'\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"vLLM (port 8082)"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl http://localhost:8082/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "mistral-7b-instruct",\n    "messages": [\n      {"role": "user", "content": "Hello! What is the capital of France?"}\n    ],\n    "max_tokens": 100\n  }\'\n'})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-openregister",children:"Integration with OpenRegister"}),"\n",(0,t.jsx)(n.h3,{id:"using-with-llphant-php",children:"Using with LLPhant (PHP)"}),"\n",(0,t.jsx)(n.p,{children:"LLPhant's OpenAI client can point to TGI/vLLM:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-php",children:"use LLPhant\\Chat\\OpenAIChat;\nuse OpenAI\\Client;\n\n// Create OpenAI client pointing to TGI/vLLM\n$client = Client::factory()\n    ->withBaseUri('http://tgi-mistral:80')  // or http://vllm-mistral:8000\n    ->withHttpHeader('Content-Type', 'application/json')\n    ->make();\n\n// Use with LLPhant\n$chat = new OpenAIChat($client);\n$response = $chat->generateText('What is the capital of France?');\n"})}),"\n",(0,t.jsx)(n.h3,{id:"configuration-in-openregister",children:"Configuration in OpenRegister"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"PHP Configuration"})," (",(0,t.jsx)(n.code,{children:"config/llm_config.php"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-php",children:"return [\n    'llm' => [\n        'provider' => 'openai',  // Use OpenAI-compatible client\n        'base_url' => 'http://tgi-mistral:80',  // Local TGI\n        // OR\n        'base_url' => 'http://vllm-mistral:8000',  // Local vLLM\n        \n        'model' => 'mistral-7b-instruct',\n        'api_key' => 'dummy',  // Not used for local, but required by some clients\n        'timeout' => 30,\n        'max_tokens' => 4096\n    ]\n];\n"})}),"\n",(0,t.jsx)(n.h3,{id:"direct-api-calls-without-llphant",children:"Direct API Calls (without LLPhant)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-php",children:"// Simple curl-based API call\nfunction callLocalLLM(string $prompt, string $baseUrl = 'http://tgi-mistral:80'): string\n{\n    $data = [\n        'model' => 'mistral-7b-instruct',\n        'messages' => [\n            ['role' => 'user', 'content' => $prompt]\n        ],\n        'max_tokens' => 1000,\n        'temperature' => 0.7\n    ];\n    \n    $ch = curl_init($baseUrl . '/v1/chat/completions');\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n    curl_setopt($ch, CURLOPT_POST, true);\n    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));\n    curl_setopt($ch, CURLOPT_HTTPHEADER, ['Content-Type: application/json']);\n    \n    $response = curl_exec($ch);\n    curl_close($ch);\n    \n    $result = json_decode($response, true);\n    return $result['choices'][0]['message']['content'] ?? '';\n}\n\n// Usage\n$answer = callLocalLLM('Explain GDPR in simple terms');\necho $answer;\n"})}),"\n",(0,t.jsx)(n.h2,{id:"available-models",children:"Available Models"}),"\n",(0,t.jsx)(n.h3,{id:"recommended-models-for-openregister",children:"Recommended Models for OpenRegister"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model"}),(0,t.jsx)(n.th,{children:"Size"}),(0,t.jsx)(n.th,{children:"Use Case"}),(0,t.jsx)(n.th,{children:"Memory Required"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Mistral-7B-Instruct-v0.2"})}),(0,t.jsx)(n.td,{children:"7B"}),(0,t.jsx)(n.td,{children:"General purpose, RAG"}),(0,t.jsx)(n.td,{children:"16GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Mixtral-8x7B-Instruct"})}),(0,t.jsx)(n.td,{children:"47B"}),(0,t.jsx)(n.td,{children:"High quality, complex"}),(0,t.jsx)(n.td,{children:"48GB+"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Llama-3-8B-Instruct"})}),(0,t.jsx)(n.td,{children:"8B"}),(0,t.jsx)(n.td,{children:"General purpose"}),(0,t.jsx)(n.td,{children:"16GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Phi-3-mini-instruct"})}),(0,t.jsx)(n.td,{children:"3.8B"}),(0,t.jsx)(n.td,{children:"Fast, lightweight"}),(0,t.jsx)(n.td,{children:"8GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Qwen2-7B-Instruct"})}),(0,t.jsx)(n.td,{children:"7B"}),(0,t.jsx)(n.td,{children:"Multilingual, code"}),(0,t.jsx)(n.td,{children:"16GB"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"changing-the-model",children:"Changing the Model"}),"\n",(0,t.jsxs)(n.p,{children:["Edit ",(0,t.jsx)(n.code,{children:"docker-compose.huggingface.yml"}),":"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"For TGI"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"tgi-mistral:\n  environment:\n    # Change this line\n    - MODEL_ID=mistralai/Mixtral-8x7B-Instruct-v0.1\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"For vLLM"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"vllm-mistral:\n  command:\n    - --model\n    - mistralai/Mixtral-8x7B-Instruct-v0.1  # Change this\n"})}),"\n",(0,t.jsx)(n.p,{children:"Then restart:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"docker-compose -f docker-compose.huggingface.yml up -d --force-recreate\n"})}),"\n",(0,t.jsx)(n.h2,{id:"openai-api-compatibility",children:"OpenAI API Compatibility"}),"\n",(0,t.jsx)(n.h3,{id:"supported-endpoints",children:"Supported Endpoints"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"TGI"})," (v1.4.0+):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/v1/chat/completions"})," - Chat completions"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/v1/completions"})," - Text completions"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/health"})," - Health check"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/info"})," - Model info"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"vLLM"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/v1/chat/completions"})," - Chat completions"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/v1/completions"})," - Text completions"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/v1/models"})," - List models"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/health"})," - Health check"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/metrics"})," - Prometheus metrics"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"compatibility-notes",children:"Compatibility Notes"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Works with"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 LLPhant (PHP) - with custom base URL"}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 OpenAI Python library - ",(0,t.jsx)(n.code,{children:'openai.base_url = "http://localhost:8081"'})]}),"\n",(0,t.jsx)(n.li,{children:"\u2705 LangChain - OpenAI integration with custom base URL"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 LlamaIndex - OpenAI LLM with custom base URL"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Any OpenAI-compatible client"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Features supported"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Streaming responses"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Temperature, top_p, max_tokens"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 System messages"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Function calling (vLLM only)"}),"\n",(0,t.jsx)(n.li,{children:"\u274c Embeddings (use separate embedding model)"}),"\n",(0,t.jsx)(n.li,{children:"\u274c Image generation (not applicable)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,t.jsx)(n.p,{children:"Both TGI and vLLM require NVIDIA GPU:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check GPU availability\nnvidia-smi\n\n# Test GPU in container\ndocker run --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi\n"})}),"\n",(0,t.jsx)(n.h3,{id:"memory-management",children:"Memory Management"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Mistral 7B Requirements"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Minimum"}),": 8GB GPU VRAM (with quantization)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Recommended"}),": 16GB GPU VRAM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Optimal"}),": 24GB GPU VRAM"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Reduce memory usage"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# TGI with quantization\ntgi-mistral:\n  environment:\n    - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\n    - QUANTIZE=bitsandbytes-nf4  # 4-bit quantization\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# vLLM with reduced memory\nvllm-mistral:\n  environment:\n    - GPU_MEMORY_UTILIZATION=0.7  # Use 70% of GPU memory\n"})}),"\n",(0,t.jsx)(n.h3,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,t.jsx)(n.p,{children:"For multiple requests:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"vLLM"})," has better batch processing:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'vllm-mistral:\n  command:\n    - --max-num-seqs\n    - "256"  # Process up to 256 sequences in parallel\n'})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"model-download-fails",children:"Model Download Fails"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check logs\ndocker logs openregister-tgi-mistral\n\n# If authentication required (gated models)\n# Add Hugging Face token\ndocker-compose -f docker-compose.huggingface.yml down\n# Edit docker-compose.huggingface.yml:\n# - HUGGING_FACE_HUB_TOKEN=hf_your_token_here\ndocker-compose -f docker-compose.huggingface.yml up -d\n"})}),"\n",(0,t.jsx)(n.h3,{id:"out-of-memory",children:"Out of Memory"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Reduce GPU memory usage\n# For vLLM, set GPU_MEMORY_UTILIZATION=0.5\n\n# OR use smaller model\n# Change to phi-3-mini-instruct (3.8B) or similar\n"})}),"\n",(0,t.jsx)(n.h3,{id:"slow-inference",children:"Slow Inference"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Check if using GPU\ndocker logs openregister-tgi-mistral | grep "CUDA"\n\n# Should see: "Using GPU: NVIDIA ..."\n# If not, GPU is not available\n\n# Ensure --gpus all flag is set in docker-compose\n'})}),"\n",(0,t.jsx)(n.h3,{id:"api-connection-refused",children:"API Connection Refused"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check if service is running\ndocker ps | grep tgi\n\n# Check if port is open\ncurl http://localhost:8081/health\n\n# Check from Nextcloud container\ndocker exec nextcloud curl http://tgi-mistral:80/health\n"})}),"\n",(0,t.jsx)(n.h2,{id:"comparison-with-ollama",children:"Comparison with Ollama"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Feature"}),(0,t.jsx)(n.th,{children:"TGI/vLLM"}),(0,t.jsx)(n.th,{children:"Ollama"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"OpenAI API"})}),(0,t.jsx)(n.td,{children:"\u2705 Yes"}),(0,t.jsx)(n.td,{children:"\u274c No (custom API)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Setup"})}),(0,t.jsx)(n.td,{children:"Medium (Docker)"}),(0,t.jsx)(n.td,{children:"Easy (one command)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Models"})}),(0,t.jsx)(n.td,{children:"Any Hugging Face"}),(0,t.jsx)(n.td,{children:"Curated list"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Performance"})}),(0,t.jsx)(n.td,{children:"\u26a1\u26a1\u26a1 Optimized"}),(0,t.jsx)(n.td,{children:"\u26a1\u26a1 Good"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"LLPhant Support"})}),(0,t.jsx)(n.td,{children:"\u2705 Yes (via OpenAI client)"}),(0,t.jsx)(n.td,{children:"\u274c Requires adapter"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Production"})}),(0,t.jsx)(n.td,{children:"\u2705 Yes"}),(0,t.jsx)(n.td,{children:"\u2705 Yes"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Recommendation"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.strong,{children:"TGI/vLLM"})," if you need OpenAI API compatibility"]}),"\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.strong,{children:"Ollama"})," if you want simpler setup and don't need OpenAI API"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"integration-examples",children:"Integration Examples"}),"\n",(0,t.jsx)(n.h3,{id:"example-1-rag-with-local-mistral",children:"Example 1: RAG with Local Mistral"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-php",children:"use LLPhant\\Chat\\OpenAIChat;\nuse LLPhant\\Embeddings\\VectorStores\\Qdrant;\n\n// Configure OpenAI client to use local TGI\n$config = [\n    'base_uri' => 'http://tgi-mistral:80',\n    'timeout' => 30\n];\n\n$chat = new OpenAIChat($config);\n\n// Use for RAG\n$context = $vectorStore->search($query, 5);\n$prompt = \"Based on this context: {$context}\\n\\nQuestion: {$query}\";\n$answer = $chat->generateText($prompt);\n"})}),"\n",(0,t.jsx)(n.h3,{id:"example-2-entity-extraction-with-local-mistral",children:"Example 2: Entity Extraction with Local Mistral"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-php",children:'function extractEntities(string $text): array\n{\n    $prompt = "Extract all person names, organizations, and locations from this text. Return as JSON.\\n\\nText: {$text}";\n    \n    $response = callLocalLLM($prompt, \'http://tgi-mistral:80\');\n    return json_decode($response, true);\n}\n\n$text = "Jan de Vries works at Gemeente Amsterdam in the Netherlands.";\n$entities = extractEntities($text);\n\n// Result:\n// {\n//   "persons": ["Jan de Vries"],\n//   "organizations": ["Gemeente Amsterdam"],\n//   "locations": ["Netherlands"]\n// }\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-3-document-summarization",children:"Example 3: Document Summarization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-php",children:"function summarizeDocument(string $content): string\n{\n    $prompt = \"Summarize the following document in 3-5 sentences:\\n\\n{$content}\";\n    return callLocalLLM($prompt, 'http://tgi-mistral:80');\n}\n"})}),"\n",(0,t.jsx)(n.h2,{id:"production-deployment",children:"Production Deployment"}),"\n",(0,t.jsx)(n.h3,{id:"resource-allocation",children:"Resource Allocation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# Production TGI configuration\ntgi-mistral:\n  deploy:\n    replicas: 2  # For high availability\n    resources:\n      limits:\n        memory: 32G\n        cpus: '8'\n      reservations:\n        memory: 16G\n        devices:\n          - driver: nvidia\n            device_ids: ['0', '1']  # Use specific GPUs\n            capabilities: [gpu]\n"})}),"\n",(0,t.jsx)(n.h3,{id:"monitoring",children:"Monitoring"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check TGI metrics\ncurl http://localhost:8081/metrics\n\n# vLLM metrics (Prometheus format)\ncurl http://localhost:8082/metrics\n"})}),"\n",(0,t.jsx)(n.h3,{id:"load-balancing",children:"Load Balancing"}),"\n",(0,t.jsx)(n.p,{children:"For multiple instances, use nginx:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-nginx",children:"upstream tgi_backend {\n    server tgi-mistral-1:80;\n    server tgi-mistral-2:80;\n}\n\nlocation /v1/ {\n    proxy_pass http://tgi_backend;\n}\n"})}),"\n",(0,t.jsx)(n.h2,{id:"related-documentation",children:"Related Documentation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/development/docker-services",children:"Docker Services Overview"})," - All included services"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/development/presidio-setup",children:"Presidio Setup"})," - Entity extraction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/Development/docker-setup",children:"Ollama Configuration"})," - Alternative local LLM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/features/ner-nlp-concepts",children:"NER & NLP Concepts"})," - Entity recognition"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"external-resources",children:"External Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://github.com/huggingface/text-generation-inference",children:"Text Generation Inference"})," - Official TGI repo"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://docs.vllm.ai/",children:"vLLM Documentation"})," - vLLM docs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://huggingface.co/models",children:"Hugging Face Models"})," - Browse models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://huggingface.co/blog/tgi-messages-api",children:"TGI Messages API Guide"})," - OpenAI compatibility"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Summary"}),": TGI and vLLM provide OpenAI-compatible APIs for local Hugging Face models like Mistral. This allows LLPhant and other OpenAI clients to use local models instead of cloud APIs, ensuring privacy and cost savings."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(a,{...e})}):a(e)}}}]);