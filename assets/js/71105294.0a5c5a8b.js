"use strict";(self.webpackChunkopen_catalogi_docs=self.webpackChunkopen_catalogi_docs||[]).push([[5034],{28453:(e,n,l)=>{l.d(n,{R:()=>t,x:()=>a});var i=l(96540);const r={},s=i.createContext(r);function t(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(s.Provider,{value:n},e.children)}},52979:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"Integrations/ollama","title":"Ollama Integration","description":"Integrate OpenRegister with Ollama to run local Large Language Models (LLMs) for AI-powered features like chat, agents, and RAG (Retrieval Augmented Generation).","source":"@site/docs/Integrations/ollama.md","sourceDirName":"Integrations","slug":"/Integrations/ollama","permalink":"/docs/Integrations/ollama","draft":false,"unlisted":false,"editUrl":"https://github.com/conductionnl/openregister/tree/main/website/docs/Integrations/ollama.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"n8n Integration","permalink":"/docs/Integrations/n8n"},"next":{"title":"Presidio Integration","permalink":"/docs/Integrations/presidio"}}');var r=l(74848),s=l(28453);const t={},a="Ollama Integration",o={},d=[{value:"Overview",id:"overview",level:2},{value:"Technical Implementation",id:"technical-implementation",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Step 1: Start Ollama Container",id:"step-1-start-ollama-container",level:3},{value:"Step 2: Pull a Model",id:"step-2-pull-a-model",level:3},{value:"Step 3: Pull Embedding Model (for RAG)",id:"step-3-pull-embedding-model-for-rag",level:3},{value:"Step 4: Configure OpenRegister",id:"step-4-configure-openregister",level:3},{value:"Configuration Details",id:"configuration-details",level:2},{value:"Ollama Service Configuration",id:"ollama-service-configuration",level:3},{value:"Accessing Ollama",id:"accessing-ollama",level:3},{value:"Model Naming",id:"model-naming",level:3},{value:"Recommended Models",id:"recommended-models",level:2},{value:"For Chat &amp; Agents",id:"for-chat--agents",level:3},{value:"For Embeddings (RAG)",id:"for-embeddings-rag",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"1. AI Chat",id:"1-ai-chat",level:3},{value:"2. RAG (Retrieval Augmented Generation)",id:"2-rag-retrieval-augmented-generation",level:3},{value:"3. Function Calling",id:"3-function-calling",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Ollama Container Won&#39;t Start",id:"ollama-container-wont-start",level:3},{value:"Model Not Found",id:"model-not-found",level:3},{value:"Connection Errors from OpenRegister",id:"connection-errors-from-openregister",level:3},{value:"Slow Performance",id:"slow-performance",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Model Loading",id:"model-loading",level:3},{value:"Parallel Requests",id:"parallel-requests",level:3},{value:"API Usage",id:"api-usage",level:2},{value:"Direct API Calls",id:"direct-api-calls",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Support",id:"support",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"ollama-integration",children:"Ollama Integration"})}),"\n",(0,r.jsx)(n.p,{children:"Integrate OpenRegister with Ollama to run local Large Language Models (LLMs) for AI-powered features like chat, agents, and RAG (Retrieval Augmented Generation)."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Ollama allows you to run open-source LLMs locally on your machine, providing:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Privacy"}),": Your data never leaves your infrastructure"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost-effective"}),": No API costs for inference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Customization"}),": Run any compatible model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Offline capability"}),": Works without internet connection"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,r.jsx)(n.p,{children:"OpenRegister uses LLPhant's native Ollama support, which provides:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Direct API Integration"}),": Uses Ollama's native ",(0,r.jsx)(n.code,{children:"/api/"})," endpoints (not OpenAI-compatible layer)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Better Performance"}),": Optimized for Ollama's specific API structure"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simpler Configuration"}),": No need for API key workarounds"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Full Feature Support"}),": Native support for chat, embeddings, and function calling"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Nextcloud 28+ with OpenRegister installed"}),"\n",(0,r.jsx)(n.li,{children:"Docker and Docker Compose"}),"\n",(0,r.jsx)(n.li,{children:"GPU recommended (8GB+ VRAM) for optimal performance"}),"\n",(0,r.jsx)(n.li,{children:"At least 16GB RAM for larger models"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-start-ollama-container",children:"Step 1: Start Ollama Container"}),"\n",(0,r.jsx)(n.p,{children:"The Ollama container is included in the docker-compose configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Start all services including Ollama\ndocker-compose up -d\n\n# Or specifically start Ollama\ndocker-compose up -d ollama\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-pull-a-model",children:"Step 2: Pull a Model"}),"\n",(0,r.jsx)(n.p,{children:"Pull one of the supported models:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Llama 3.2 (8B) - Recommended, best balance\ndocker exec openregister-ollama ollama pull llama3.2\n\n# Llama 3.2 (3B) - Lighter alternative\ndocker exec openregister-ollama ollama pull llama3.2:3b\n\n# Mistral (7B) - Fast and efficient\ndocker exec openregister-ollama ollama pull mistral:7b\n\n# Phi-3 Mini (3.8B) - Lightweight option\ndocker exec openregister-ollama ollama pull phi3:mini\n\n# CodeLlama (7B) - Optimized for code\ndocker exec openregister-ollama ollama pull codellama:latest\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-pull-embedding-model-for-rag",children:"Step 3: Pull Embedding Model (for RAG)"}),"\n",(0,r.jsx)(n.p,{children:"If using RAG features, pull an embedding model:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Nomic Embed Text - Recommended for embeddings\ndocker exec openregister-ollama ollama pull nomic-embed-text:latest\n\n# Alternative: all-minilm (smaller, faster)\ndocker exec openregister-ollama ollama pull all-minilm:latest\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-4-configure-openregister",children:"Step 4: Configure OpenRegister"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Navigate to ",(0,r.jsx)(n.strong,{children:"Settings"})," \u2192 ",(0,r.jsx)(n.strong,{children:"OpenRegister"})," \u2192 ",(0,r.jsx)(n.strong,{children:"LLM Configuration"})]}),"\n",(0,r.jsxs)(n.li,{children:["Select ",(0,r.jsx)(n.strong,{children:"Ollama"})," as the chat provider"]}),"\n",(0,r.jsx)(n.li,{children:"Configure the settings:"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Integrated Setup (docker-compose.yml):"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ollama URL"}),": ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})," \u26a0\ufe0f ",(0,r.jsx)(n.strong,{children:"NOT"})," ",(0,r.jsx)(n.code,{children:"http://localhost:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chat Model"}),": ",(0,r.jsx)(n.code,{children:"llama3.2:latest"})," (use full name including tag)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding Model"}),": ",(0,r.jsx)(n.code,{children:"nomic-embed-text:latest"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why not localhost?"})," OpenRegister runs inside Nextcloud container; use container name instead"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Standalone Setup:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Via Docker Network"}),": ",(0,r.jsx)(n.code,{children:"http://standalone-ollama:11434"})," (after connecting networks)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Via Host IP"}),": ",(0,r.jsx)(n.code,{children:"http://YOUR_HOST_IP:11434"})," (e.g., ",(0,r.jsx)(n.code,{children:"http://192.168.1.100:11434"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"NOT"}),": ",(0,r.jsx)(n.code,{children:"http://localhost:11434"})," (won't work from inside container)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"configuration-details",children:"Configuration Details"}),"\n",(0,r.jsx)(n.h3,{id:"ollama-service-configuration",children:"Ollama Service Configuration"}),"\n",(0,r.jsxs)(n.p,{children:["The Ollama service is configured in ",(0,r.jsx)(n.code,{children:"docker-compose.yml"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'ollama:\n  image: ollama/ollama:latest\n  container_name: openregister-ollama\n  restart: always\n  ports:\n    - "11434:11434"\n  volumes:\n    - ollama:/root/.ollama\n  environment:\n    - OLLAMA_HOST=0.0.0.0\n    - OLLAMA_NUM_PARALLEL=4\n    - OLLAMA_KEEP_ALIVE=30m\n  deploy:\n    resources:\n      limits:\n        memory: 16G\n      reservations:\n        memory: 8G\n        devices:\n          - driver: nvidia\n            count: all\n            capabilities: [gpu]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"accessing-ollama",children:"Accessing Ollama"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Important: Docker Container Communication"})}),"\n",(0,r.jsxs)(n.p,{children:["When configuring Ollama in OpenRegister, you MUST use the Docker service name, not ",(0,r.jsx)(n.code,{children:"localhost"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Correct (from Nextcloud container)"}),": ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})," or ",(0,r.jsx)(n.code,{children:"http://ollama:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"Wrong"}),": ",(0,r.jsx)(n.code,{children:"http://localhost:11434"})," (this only works from your host machine, not from inside containers)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Access Points:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"From host machine"})," (terminal, browser): ",(0,r.jsx)(n.code,{children:"http://localhost:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"From Nextcloud container"})," (OpenRegister settings): ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"From other Docker containers"}),": ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why?"})," Inside a Docker container, ",(0,r.jsx)(n.code,{children:"localhost"})," refers to the container itself, not your host machine. Containers communicate with each other using service names defined in docker-compose.yml."]}),"\n",(0,r.jsx)(n.h3,{id:"model-naming",children:"Model Naming"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Important"}),": Ollama models must be referenced with their full name including version tags:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Correct"}),": ",(0,r.jsx)(n.code,{children:"mistral:7b"}),", ",(0,r.jsx)(n.code,{children:"llama3.2:latest"}),", ",(0,r.jsx)(n.code,{children:"phi3:mini"})]}),"\n",(0,r.jsxs)(n.li,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"Wrong"}),": ",(0,r.jsx)(n.code,{children:"mistral"}),", ",(0,r.jsx)(n.code,{children:"llama3.2"}),", ",(0,r.jsx)(n.code,{children:"phi3"})," (without tags)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The OpenRegister UI dropdown shows full model names with tags to ensure compatibility."}),"\n",(0,r.jsx)(n.h2,{id:"recommended-models",children:"Recommended Models"}),"\n",(0,r.jsx)(n.h3,{id:"for-chat--agents",children:"For Chat & Agents"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Size"}),(0,r.jsx)(n.th,{children:"RAM Required"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"Quality"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Llama 3.2 (8B)"})}),(0,r.jsx)(n.td,{children:"8B"}),(0,r.jsx)(n.td,{children:"16GB"}),(0,r.jsx)(n.td,{children:"\u26a1\u26a1"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"General purpose, best balance"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Llama 3.2 (3B)"})}),(0,r.jsx)(n.td,{children:"3B"}),(0,r.jsx)(n.td,{children:"8GB"}),(0,r.jsx)(n.td,{children:"\u26a1\u26a1\u26a1"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"Fast, lightweight"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Mistral (7B)"})}),(0,r.jsx)(n.td,{children:"7B"}),(0,r.jsx)(n.td,{children:"16GB"}),(0,r.jsx)(n.td,{children:"\u26a1\u26a1\u26a1"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"Fast and efficient"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Phi-3 Mini"})}),(0,r.jsx)(n.td,{children:"3.8B"}),(0,r.jsx)(n.td,{children:"8GB"}),(0,r.jsx)(n.td,{children:"\u26a1\u26a1\u26a1"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"Very fast, good quality"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"CodeLlama"})}),(0,r.jsx)(n.td,{children:"7B"}),(0,r.jsx)(n.td,{children:"16GB"}),(0,r.jsx)(n.td,{children:"\u26a1\u26a1"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"Code generation, technical"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"for-embeddings-rag",children:"For Embeddings (RAG)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Size"}),(0,r.jsx)(n.th,{children:"Dimensions"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"Quality"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"nomic-embed-text"})}),(0,r.jsx)(n.td,{children:"137M"}),(0,r.jsx)(n.td,{children:"768"}),(0,r.jsx)(n.td,{children:"\u26a1\u26a1\u26a1"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"General purpose, recommended"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"all-minilm"})}),(0,r.jsx)(n.td,{children:"22M"}),(0,r.jsx)(n.td,{children:"384"}),(0,r.jsx)(n.td,{children:"\u26a1\u26a1\u26a1\u26a1"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"Fast, smaller vectors"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsx)(n.h3,{id:"1-ai-chat",children:"1. AI Chat"}),"\n",(0,r.jsx)(n.p,{children:"Enable conversational AI in OpenRegister:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-php",children:"// Configured via Settings \u2192 LLM Configuration\n// Select Ollama as provider\n// Choose chat model (e.g., llama3.2:latest)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-rag-retrieval-augmented-generation",children:"2. RAG (Retrieval Augmented Generation)"}),"\n",(0,r.jsx)(n.p,{children:"Answer questions using your data:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Configure embedding model: ",(0,r.jsx)(n.code,{children:"nomic-embed-text:latest"})]}),"\n",(0,r.jsx)(n.li,{children:"Vectorize your objects and files"}),"\n",(0,r.jsx)(n.li,{children:"Ask questions - AI retrieves relevant context and answers"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-function-calling",children:"3. Function Calling"}),"\n",(0,r.jsx)(n.p,{children:"Use Ollama with OpenRegister's function calling capabilities:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Search objects"}),"\n",(0,r.jsx)(n.li,{children:"Create objects"}),"\n",(0,r.jsx)(n.li,{children:"Update objects"}),"\n",(0,r.jsx)(n.li,{children:"Query registers"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"ollama-container-wont-start",children:"Ollama Container Won't Start"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check logs\ndocker logs openregister-ollama\n\n# Common issues:\n# 1. Port 11434 already in use\nsudo lsof -i :11434\n\n# 2. Insufficient memory\ndocker stats openregister-ollama\n\n# 3. GPU not available\ndocker exec openregister-ollama nvidia-smi\n"})}),"\n",(0,r.jsx)(n.h3,{id:"model-not-found",children:"Model Not Found"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# List available models\ndocker exec openregister-ollama ollama list\n\n# Pull missing model\ndocker exec openregister-ollama ollama pull llama3.2:latest\n\n# Verify model name includes tag\ndocker exec openregister-ollama ollama show llama3.2:latest\n"})}),"\n",(0,r.jsx)(n.h3,{id:"connection-errors-from-openregister",children:"Connection Errors from OpenRegister"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem"}),": OpenRegister can't connect to Ollama."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Verify Ollama URL uses container name: ",(0,r.jsx)(n.code,{children:"http://openregister-ollama:11434"})]}),"\n",(0,r.jsxs)(n.li,{children:["Check containers are on same Docker network: ",(0,r.jsx)(n.code,{children:"docker network ls"})]}),"\n",(0,r.jsxs)(n.li,{children:["Test connection from Nextcloud container:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker exec <nextcloud-container> curl http://openregister-ollama:11434/api/tags\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"slow-performance",children:"Slow Performance"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Use GPU acceleration (10-100x faster)"}),"\n",(0,r.jsx)(n.li,{children:"Choose smaller model (3B instead of 8B)"}),"\n",(0,r.jsxs)(n.li,{children:["Increase ",(0,r.jsx)(n.code,{children:"OLLAMA_KEEP_ALIVE"})," to keep models loaded"]}),"\n",(0,r.jsx)(n.li,{children:"Use quantization (smaller model variants)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,r.jsx)(n.p,{children:"For best performance, use GPU:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"deploy:\n  resources:\n    devices:\n      - driver: nvidia\n        count: all\n        capabilities: [gpu]\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance Gain"}),": 10-100x faster inference with GPU"]}),"\n",(0,r.jsx)(n.h3,{id:"model-loading",children:"Model Loading"}),"\n",(0,r.jsx)(n.p,{children:"Keep models loaded in memory:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"environment:\n  - OLLAMA_KEEP_ALIVE=30m  # Keep models loaded for 30 minutes\n"})}),"\n",(0,r.jsx)(n.h3,{id:"parallel-requests",children:"Parallel Requests"}),"\n",(0,r.jsx)(n.p,{children:"Handle multiple requests:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"environment:\n  - OLLAMA_NUM_PARALLEL=4  # Process 4 requests simultaneously\n"})}),"\n",(0,r.jsx)(n.h2,{id:"api-usage",children:"API Usage"}),"\n",(0,r.jsx)(n.h3,{id:"direct-api-calls",children:"Direct API Calls"}),"\n",(0,r.jsx)(n.p,{children:"Test Ollama directly:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# List models\ncurl http://localhost:11434/api/tags\n\n# Chat completion\ncurl http://localhost:11434/api/chat -d \'{\n  "model": "llama3.2:latest",\n  "messages": [\n    {"role": "user", "content": "Hello!"}\n  ]\n}\'\n\n# Generate embeddings\ncurl http://localhost:11434/api/embeddings -d \'{\n  "model": "nomic-embed-text:latest",\n  "prompt": "Your text here"\n}\'\n'})}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://ollama.ai/docs",children:"Ollama Official Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/docs/development/ollama",children:"LLPhant Ollama Integration"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/docs/features/rag-implementation",children:"RAG Implementation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/docs/features/ai",children:"AI Chat Features"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"support",children:"Support"}),"\n",(0,r.jsx)(n.p,{children:"For issues specific to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ollama setup"}),": Check ",(0,r.jsx)(n.a,{href:"https://ollama.ai/docs",children:"Ollama Documentation"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenRegister integration"}),": OpenRegister GitHub issues"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model selection"}),": See recommended models above"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);