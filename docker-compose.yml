version: "3.5"
volumes:
  nextcloud:
  apps:
  db:
  config:
  solr:
  zookeeper:
  ollama:

services:
  db:
    image: mariadb:10.6
    restart: always
    command: --transaction-isolation=READ-COMMITTED --log-bin=binlog --binlog-format=ROW
    volumes:
      - db:/var/lib/mysql
    environment:
      - MYSQL_ROOT_PASSWORD='!ChangeMe!'
      - MYSQL_PASSWORD='!ChangeMe!'
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud

  # ZooKeeper for SolrCloud coordination
  zookeeper:
    image: zookeeper:3.8
    container_name: openregister-zookeeper
    restart: always
    environment:
      - ZOO_MY_ID=1
      - ZOO_SERVERS=server.1=0.0.0.0:2888:3888;2181
    volumes:
      - zookeeper:/data
    ports:
      - "2181:2181"

  # SOLR in SolrCloud mode
  solr:
    image: solr:9-slim
    container_name: openregister-solr
    restart: always
    ports:
      - "8983:8983"
    volumes:
      - solr:/var/solr
    environment:
      - SOLR_HEAP=512m
      - ZK_HOST=zookeeper:2181
    depends_on:
      - zookeeper
    command:
      - bash
      - -c
      - |
        # Wait for ZooKeeper to be ready
        echo "Waiting for ZooKeeper..."
        while ! nc -z zookeeper 2181; do sleep 1; done
        echo "ZooKeeper is ready!"
        
        # Start SOLR in SolrCloud mode
        solr-foreground -c -z zookeeper:2181
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8983/solr/admin/info/system || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Ollama for local LLM inference
  # Provides local AI models for chat, RAG, and agent functionality
  # Access at: http://localhost:11434
  # Pull models: docker exec openregister-ollama ollama pull llama3.2
  ollama:
    image: ollama/ollama:latest
    container_name: openregister-ollama
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      # Allow multiple concurrent requests
      - OLLAMA_NUM_PARALLEL=2
      # Keep models loaded for faster responses
      - OLLAMA_KEEP_ALIVE=15m
    # Memory limits for larger models (Llama 3.2 8B, Mistral, etc.)
    # GPU support enabled for NVIDIA GPUs
    deploy:
      resources:
        limits:
          memory: 16G  # Sufficient for 8B models
        reservations:
          memory: 8G   # Minimum required
          devices:
            - driver: nvidia
              count: all  # Use all available GPUs
              capabilities: [gpu]
    # Shared memory for model loading
    shm_size: '2gb'
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # init-ubuntu:
  #   image: ubuntu
  #   command: sh /home/ubuntu/docker/init-ubuntu.sh
  #   volumes:
  #     - ./docker:/home/ubuntu/docker
  #     - .:/home/ubuntu/app

  nextcloud:
    user: root
    container_name: nextcloud
#    entrypoint: occ app:enable openregister
    image: nextcloud
    restart: always
    ports:
      - 8080:80
    links:
      - db
      - solr
      - ollama
    volumes:
      - nextcloud:/var/www/html:rw
      - ./custom_apps:/var/www/html/custom_apps
      - .:/var/www/html/custom_apps/openregister
    environment:
      - MYSQL_PASSWORD='!ChangeMe!'
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
      - MYSQL_HOST=db
      - TZ=Europe/Amsterdam
      - NEXTCLOUD_ADMIN_USER=admin
      - NEXTCLOUD_ADMIN_PASSWORD=admin
      # PHP Configuration - Match production settings
      - PHP_MEMORY_LIMIT=4G
      - PHP_UPLOAD_LIMIT=2G
      - PHP_POST_MAX_SIZE=2G
    # depends_on:
    #   init-ubuntu:
    #     condition: service_completed_successfully
