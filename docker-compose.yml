volumes:
  nextcloud:
  apps:
  db:
  config:
  ollama:
  presidio-models:
  n8n:
  tgi-models:
  openllm-models:
  openllm-cache:
  solr:
  zookeeper:
  elasticsearch:

services:
  # PostgreSQL Database (default)
  # Start with: docker-compose up (default) OR docker-compose --profile postgres up
  # Recommended for production use with vector search capabilities
  db:
    # Empty profiles means it starts by default unless another profile overrides it
    image: pgvector/pgvector:pg16
    restart: always
    container_name: openregister-postgres
    volumes:
      - db:/var/lib/postgresql/data
      - ./docker/postgres/init-extensions.sql:/docker-entrypoint-initdb.d/01-init-extensions.sql:ro
    environment:
      - POSTGRES_DB=nextcloud
      - POSTGRES_USER=nextcloud
      - POSTGRES_PASSWORD=!ChangeMe!
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U nextcloud -d nextcloud"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: >
      postgres
      -c shared_preload_libraries=pg_trgm,vector
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=4MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB

  # MariaDB Database (optional - for compatibility testing)
  # Start with: docker-compose --profile mariadb up
  # Note: Vector search features will not be available with MariaDB
  db-mariadb:
    profiles:
      - mariadb
    image: mariadb:11.2
    restart: always
    container_name: openregister-mariadb
    volumes:
      - db:/var/lib/mysql
    environment:
      - MYSQL_ROOT_PASSWORD=!ChangeMe!
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
      - MYSQL_PASSWORD=!ChangeMe!
    ports:
      - "3306:3306"
    command: >
      --transaction-isolation=READ-COMMITTED
      --log-bin=binlog
      --binlog-format=ROW
      --innodb-file-per-table=1
      --max-connections=200
      --innodb-buffer-pool-size=256M
      --innodb-log-file-size=64M
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ZooKeeper for SolrCloud coordination (Optional - use --profile solr)
  zookeeper:
    profiles:
      - solr
      - search
    image: zookeeper:3.8
    container_name: openregister-zookeeper
    restart: always
    environment:
      - ZOO_MY_ID=1
      - ZOO_SERVERS=server.1=0.0.0.0:2888:3888;2181
    volumes:
      - zookeeper:/data
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "echo stat | nc localhost 2181"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Solr in SolrCloud mode (Optional - use --profile solr)
  # Traditional search engine with advanced features
  # Access at: http://localhost:8983
  # Note: PostgreSQL with pgvector+pg_trgm is now the recommended approach
  solr:
    profiles:
      - solr
      - search
    image: solr:9-slim
    container_name: openregister-solr
    restart: always
    ports:
      - "8983:8983"
    volumes:
      - solr:/var/solr
    environment:
      - SOLR_HEAP=512m
      - ZK_HOST=zookeeper:2181
    depends_on:
      - zookeeper
    command:
      - bash
      - -c
      - |
        # Wait for ZooKeeper to be ready.
        echo "Waiting for ZooKeeper..."
        while ! nc -z zookeeper 2181; do sleep 1; done
        echo "ZooKeeper is ready!"
        
        # Start Solr in SolrCloud mode.
        solr-foreground -c -z zookeeper:2181
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8983/solr/admin/info/system || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Elasticsearch for alternative search backend (Optional - use --profile elasticsearch)
  # Modern search and analytics engine
  # Access at: http://localhost:9200
  # Note: PostgreSQL with pgvector+pg_trgm is now the recommended approach
  elasticsearch:
    profiles:
      - elasticsearch
      - search
    image: elasticsearch:8.11.3
    container_name: openregister-elasticsearch
    restart: always
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch:/usr/share/elasticsearch/data
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - cluster.name=openregister-cluster
      - node.name=openregister-node-1
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Ollama for local LLM inference
  # Provides local AI models for chat, RAG, and agent functionality
  # Access at: http://localhost:11434
  # Pull models: docker exec openregister-ollama ollama pull llama3.2
  ollama:
    image: ollama/ollama:latest
    container_name: openregister-ollama
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      # Allow multiple concurrent requests
      - OLLAMA_NUM_PARALLEL=2
      # Keep models loaded for faster responses
      - OLLAMA_KEEP_ALIVE=15m
    # Memory limits for larger models (Llama 3.2 8B, Mistral, etc.)
    # GPU support enabled for NVIDIA GPUs
    deploy:
      resources:
        limits:
          memory: 16G  # Sufficient for 8B models
        reservations:
          memory: 8G   # Minimum required
          devices:
            - driver: nvidia
              count: all  # Use all available GPUs
              capabilities: [gpu]
    # Shared memory for model loading
    shm_size: '2gb'
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Presidio Analyzer for PII detection and NER
  # Microsoft's open-source PII detection service (RECOMMENDED FOR PRODUCTION)
  # Access at: http://localhost:5001
  # Documentation: https://microsoft.github.io/presidio/
  presidio-analyzer:
    image: mcr.microsoft.com/presidio-analyzer:latest
    container_name: openregister-presidio-analyzer
    restart: always
    ports:
      - "5001:5001"
    environment:
      - GRPC_PORT=5001
      - LOG_LEVEL=INFO
      # Enable multi-language support
      - PRESIDIO_ANALYZER_LANGUAGES=en,nl,de,fr,es
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # n8n Workflow Automation (Optional - use --profile n8n)
  # Integrates with OpenRegister via Nextcloud webhooks
  # Access at: http://localhost:5678
  # Default credentials: admin / admin (change in production!)
  # Documentation: https://docs.n8n.io
  # MCP Integration: See website/docs/technical/n8n-mcp/setup.md
  n8n:
    profiles:
      - n8n
      - automation
    image: n8nio/n8n:latest
    container_name: openregister-n8n
    restart: always
    user: root
    ports:
      - "5678:5678"
    volumes:
      - n8n:/root/.n8n
      - /var/run/docker.sock:/var/run/docker.sock
      # Mount n8n-mcp configuration and data
      - ./n8n-mcp:/usr/local/lib/n8n-mcp:ro
    environment:
      # Database configuration (PostgreSQL)
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=openregister-postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=n8n
      - DB_POSTGRESDB_USER=nextcloud
      - DB_POSTGRESDB_PASSWORD=!ChangeMe!
      # Basic authentication
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=admin
      # Host configuration
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=http://localhost:5678/
      # Timezone
      - GENERIC_TIMEZONE=Europe/Amsterdam
      - TZ=Europe/Amsterdam
      # Execution configuration
      - EXECUTIONS_PROCESS=main
      - EXECUTIONS_DATA_SAVE_ON_ERROR=all
      - EXECUTIONS_DATA_SAVE_ON_SUCCESS=all
      - EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS=true
      # Workflow settings
      - WORKFLOWS_DEFAULT_NAME=My Workflow
      - N8N_DIAGNOSTICS_ENABLED=false
      # API configuration
      - N8N_API_KEYS_ENABLED=true
    depends_on:
      - db
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://localhost:5678/healthz || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Hugging Face Text Generation Inference (TGI) with OpenAI-compatible API (Optional - use --profile huggingface)
  # Provides high-performance inference for language models
  # Access at: http://localhost:8081
  # OpenAI-compatible API endpoint: http://localhost:8081/v1
  # Documentation: https://huggingface.co/docs/text-generation-inference
  tgi-llm:
    profiles:
      - huggingface
      - llm
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: openregister-tgi-llm
    restart: always
    ports:
      - "8081:80"
    volumes:
      - tgi-models:/data
    environment:
      # Model configuration - Change to your preferred model
      - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2
      # Alternative models:
      # - MODEL_ID=meta-llama/Llama-2-7b-chat-hf
      # - MODEL_ID=codellama/CodeLlama-7b-Instruct-hf
      # - MODEL_ID=teknium/OpenHermes-2.5-Mistral-7B
      
      # Performance settings
      - MAX_INPUT_LENGTH=4096
      - MAX_TOTAL_TOKENS=8192
      - MAX_BATCH_PREFILL_TOKENS=4096
      - MAX_CONCURRENT_REQUESTS=128
      - MAX_WAITING_TOKENS=20
      
      # Quantization (reduce memory usage)
      # - QUANTIZE=bitsandbytes-nf4
      
      # Hugging Face token (required for gated models like Llama)
      # Get token from: https://huggingface.co/settings/tokens
      # - HUGGING_FACE_HUB_TOKEN=your_token_here
      
      # Sharding for multi-GPU (if available)
      # - NUM_SHARD=2
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: '2gb'
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # OpenLLM Management Interface (Optional - use --profile huggingface)
  # Web UI for managing and downloading language models
  # Access at: http://localhost:3000
  # Documentation: https://github.com/bentoml/OpenLLM
  openllm:
    profiles:
      - huggingface
      - llm
    image: ghcr.io/bentoml/openllm:latest
    container_name: openregister-openllm
    restart: always
    ports:
      - "3000:3000"
      - "8082:8082"
    volumes:
      - openllm-models:/models
      - openllm-cache:/root/.cache
    environment:
      # Model configuration
      - OPENLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
      - OPENLLM_BACKEND=vllm
      - OPENLLM_PORT=3000
      - OPENLLM_API_PORT=8082
      
      # GPU configuration
      - CUDA_VISIBLE_DEVICES=0
      
      # Performance settings
      - OPENLLM_MAX_MODEL_LEN=4096
      - OPENLLM_GPU_MEMORY_UTILIZATION=0.9
      
      # Hugging Face token
      # - HUGGING_FACE_HUB_TOKEN=your_token_here
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: '2gb'
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    command: start mistralai/Mistral-7B-Instruct-v0.2 --backend vllm

  # init-ubuntu:
  #   image: ubuntu
  #   command: sh /home/ubuntu/docker/init-ubuntu.sh
  #   volumes:
  #     - ./docker:/home/ubuntu/docker
  #     - .:/home/ubuntu/app

  # Nextcloud Application Server (PostgreSQL - Default)
  # Start with: docker-compose up  OR  docker-compose --profile postgres up
  # Connects to PostgreSQL database
  # NOTE: When using --profile mariadb, this service will NOT start (nextcloud-mariadb starts instead)
  nextcloud:
    user: root
    container_name: nextcloud
#    entrypoint: occ app:enable openregister
    image: nextcloud
    restart: always
    ports:
      - 8080:80
    links:
      - db
      - ollama
      - presidio-analyzer
    volumes:
      - nextcloud:/var/www/html:rw
      - ./custom_apps:/var/www/html/custom_apps
      - .:/var/www/html/custom_apps/openregister
    environment:
      # Database configuration (PostgreSQL)
      - POSTGRES_DB=nextcloud
      - POSTGRES_USER=nextcloud
      - POSTGRES_PASSWORD=!ChangeMe!
      - POSTGRES_HOST=db
      - TZ=Europe/Amsterdam
      - NEXTCLOUD_ADMIN_USER=admin
      - NEXTCLOUD_ADMIN_PASSWORD=admin
      # PHP Configuration - Match production settings
      - PHP_MEMORY_LIMIT=4G
      - PHP_UPLOAD_LIMIT=2G
      - PHP_POST_MAX_SIZE=2G

  # Nextcloud Application Server (MariaDB - For Testing)
  # Start with: docker-compose --profile mariadb up
  # Note: Do NOT use docker-compose up when using this profile
  # Always specify: docker-compose --profile mariadb up
  nextcloud-mariadb:
    profiles:
      - mariadb
    user: root
    container_name: nextcloud-mariadb
    image: nextcloud
    restart: always
    ports:
      - 8080:80
    links:
      - db-mariadb
      - ollama
      - presidio-analyzer
    volumes:
      - nextcloud:/var/www/html:rw
      - ./custom_apps:/var/www/html/custom_apps
      - .:/var/www/html/custom_apps/openregister
    environment:
      # Database configuration (MariaDB)
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
      - MYSQL_PASSWORD=!ChangeMe!
      - MYSQL_HOST=db-mariadb
      - TZ=Europe/Amsterdam
      - NEXTCLOUD_ADMIN_USER=admin
      - NEXTCLOUD_ADMIN_PASSWORD=admin
      # PHP Configuration - Match production settings
      - PHP_MEMORY_LIMIT=4G
      - PHP_UPLOAD_LIMIT=2G
      - PHP_POST_MAX_SIZE=2G
    depends_on:
      db-mariadb:
        condition: service_healthy
